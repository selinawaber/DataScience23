{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas_datareader import data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b155f9",
   "metadata": {},
   "source": [
    "## Loading the Data Set\n",
    "\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    project_directory = sys.path[0] ## get path of project directory\n",
    "    data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "    years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "    ## Loading the yearly dataset into the array dfs\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "        df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "        df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "        df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "        df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True) ## concat the diffrent dataframes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c96c66",
   "metadata": {},
   "source": [
    "## Some Explanation of Variables:\n",
    "\n",
    "### Adding `year` as a categorical variable\n",
    "\n",
    "We added a column named year which contains the respecitve year.\n",
    "\n",
    "### Handling the variable `Price VAR [%]`\n",
    "\n",
    "The last column, `PRICE VAR [%]`, lists the percent price variation of each stock for the year. For example, if we consider the dataset 2015_Financial_Data.csv, we will have:\n",
    "\n",
    "- 200+ financial indicators for the year 2015;\n",
    "- percent price variation for the year 2016 (meaning from the first trading day on Jan 2016 to the last trading day on Dec 2016).\n",
    "\n",
    "We renamed all the variables with the specific year in it, e.g. `2016 PRICE VAR [%]` to `PRICE VAR [%]`. We dropped the old ones. \n",
    "\n",
    "### the variable `class`\n",
    "\n",
    "class lists a binary classification for each stock, where\n",
    "\n",
    "- for each stock, if the PRICE VAR [%] value is positive, class = 1. From a trading perspective, the 1 identifies those stocks that an hypothetical trader should BUY at the start of the year and sell at the end of the year for a profit.\n",
    "- for each stock, if the PRICE VAR [%] value is negative, class = 0. From a trading perspective, the 0 identifies those stocks that an hypothetical trader should NOT BUY, since their value will decrease, meaning a loss of capital.\n",
    "\n",
    "The columns `PRICE VAR [%]` and `class` make possible to use the datasets for both classification and regression tasks:\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to classify those stocks that in buy-worthy and not buy-worthy, it is possible to get the targets from the class column;\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to predict the future value of a stock, it is possible to get the targets from the PRICE VAR [%] column.\n",
    "\n",
    "### the variable `Stock Name`\n",
    "\n",
    "We named the first variable Stock Namesince it has not been named in the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df450a48",
   "metadata": {},
   "source": [
    "## Numerical and Catgorical Features/Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are converting Classto a cathegorical variable.\n",
    "def class_to_categorical(df):\n",
    "    df['class'] = df.Class.astype('object') ## object or catheogry?? whats the difference??\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_numerical_categorical_variables(df):\n",
    "    \n",
    "    numCols = df.select_dtypes(exclude='object').columns\n",
    "    print(f\"There are {len(numCols)} numerical features:\\n\")\n",
    "\n",
    "    catCols = df.select_dtypes(include='object').columns\n",
    "    print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbd0e7",
   "metadata": {},
   "source": [
    "## Any Duplicates? \n",
    "\n",
    "No, there are no duplicates for rows but there are 20 duplicates for columns/ 10 each. Not same variable name but same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_row(df):\n",
    "    print(f'Duplicates in Rows:', True in list(df.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_col(df):\n",
    "    print(f'Duplicates in Columns:', True in list(df.T.duplicated().T))\n",
    "    print(\"Show the Duplicates:\")\n",
    "    print(df.T[df.T.duplicated(keep=False)].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df,columns):\n",
    "    shape_old=df.shape\n",
    "\n",
    "    df=df.drop(columns=columns)\n",
    "\n",
    "    print(f' Shape with duplicates:', shape_old) \n",
    "    print(f' Shape after removal of duplicates:', df.shape) \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfdaa81",
   "metadata": {},
   "source": [
    "Our Duplicates are the following pairs:\n",
    "\n",
    "- `ebitperRevenue` and `eBITperRevenu`\n",
    "- `ebtperEBIT` and `eBTperEBIT`\n",
    "- `niperEBT` and `nIperEBT`\n",
    "- `returnOnAssets` and `Return on Tangible Assets`\n",
    "- `returnOnCapitalEmployed` and `ROIC`\n",
    "- `payablesTurnover` and `Payables Turnover`\n",
    "- `inventoryTurnover` and `Inventory Turnover`\n",
    "- `debtRatio` and `Debt to Assets`\n",
    "- `debtEquityRatio` and `Debt to Equity`\n",
    "- `cashFlowToDebtRatio` and `cashFlowCoverageRatios`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985d72c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f32c4",
   "metadata": {},
   "source": [
    "We got the data from the following webpages: [S&P means](https://www.macrotrends.net/2526/sp-500-historical-annual-returns) and [inflation](https://www.macrotrends.net/countries/USA/united-states/inflation-rate-cpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52605ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_indicators(df):\n",
    "    \n",
    "    ## Yearly Means of S&P 500\n",
    "    sp500_means = pd.Series([11.39, -0.73, 9.54, 19.42, -6.24], index = [2014, 2015, 2016, 2017, 2018]) ## or should it start with year 2015 to year 2019???\n",
    "    ## for year 2019 we got 28.88%\n",
    "    \n",
    "    # Yearly Inflaction Rate measured by consumer price index\n",
    "    inflation = pd.Series([1.62, 0.12, 1.26, 2.13, 2.44], index = [2014, 2015, 2016, 2017, 2018]) ## or should it start with year 2015 to year 2019???\n",
    "    ## or should we look at annual change????\n",
    "    \n",
    "    ##Adding to the dataframe\n",
    "    df[\"inflation\"] = df.apply(lambda x: inflation[int(x[\"year\"])], axis=1)\n",
    "    df[\"sp500_means\"] = df.apply(lambda x: sp500_means[int(x[\"year\"])], axis=1)\n",
    "    \n",
    "    \n",
    "    ## Calculation of Excess Return:\n",
    "    df[\"excess_return\"] = np.subtract(df[\"PRICE VAR [%]\"], df[\"sp500_means\"])\n",
    "\n",
    "\n",
    "    ## Calculation of Cashflow Margin:\n",
    "    df[\"cashflow_margin\"] = df[\"Operating Cash Flow\"].divide(df[\"Revenue\"])\n",
    "    # Pay attention to ZeroDivisionError, replace infinity by NAN\n",
    "    df[\"cashflow_margin\"] = df[\"cashflow_margin\"].replace([np.inf, -np.inf], np.nan)\n",
    "   \n",
    "    \n",
    "    ## Calculation of Return on Net Assets (RONA)\n",
    "    df[\"Net_working_capital\"] = df[\"Total assets\"]-df[\"Cash and cash equivalents\"]\n",
    "    df[\"RONA\"] = df[\"EBIT\"]/df[\"Net_working_capital\"]\n",
    "    df[\"RONA\"] = df[\"RONA\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "    #df[\"operating_liabilities\"] = df[\"Total liabilities\"]-df[\"Total debt\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_indicators(df):\n",
    "    df_new = df.drop([\"operatingProfitMargin\"], axis = 1) # consisting only of the value 1.\n",
    "    \n",
    "    #maybe more to drop? which are not yet addressed in correlation or duplicates or elsewhere?\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0c759",
   "metadata": {},
   "source": [
    "## Correlation of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation(df):\n",
    "    X = df[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "    y = df[\"Class\"]\n",
    "    plt.matshow(X.corr().abs())\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    abs_corr = X.corr().abs()\n",
    "    for i in range(len(abs_corr)):\n",
    "        abs_corr.iloc[i, i] = 0\n",
    "        \n",
    "    abs_corr_unstack = abs_corr.unstack()\n",
    "    abs_corr_unstack.sort_values(kind=\"quicksort\")[-50:]\n",
    "\n",
    "    print((abs_corr_unstack.values>0.99).sum()/2)\n",
    "\n",
    "    return abs_corr_unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suggestion to deal with the correlations: remove a variable if its correlation with another variable is higher than 0.99\n",
    "def remove_correlation(df, abs_corr_unstack):\n",
    "    columns_to_drop = []\n",
    "    columns_to_remain = []\n",
    "\n",
    "    for pair in abs_corr_unstack.index.values:\n",
    "        if abs_corr_unstack[pair] > 0.99:\n",
    "            if pair[0] not in columns_to_remain and pair[1] not in columns_to_remain:\n",
    "                    columns_to_remain.append(pair[0])\n",
    "                    if pair[1] not in columns_to_drop:\n",
    "                        columns_to_drop.append(pair[1])\n",
    "            elif pair[0] in columns_to_remain:\n",
    "                if pair[1] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[1])\n",
    "            elif pair[1] in columns_to_remain:\n",
    "                if pair[0] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[0])\n",
    "\n",
    "    df_corr_removed = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df_corr_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49a06f",
   "metadata": {},
   "source": [
    "## Class Balance?\n",
    "\n",
    "The Variable `Class`is not balanced. We have to keep that in mind for train and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_imbalance(y):\n",
    "    sns.countplot(x=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169a0c0",
   "metadata": {},
   "source": [
    "## Outliers Dedection for `PRICE VAR[%]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39096800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_sectors(df):\n",
    "    df_ = df.loc[:, ['Sector','PRICE VAR [%]']]\n",
    "\n",
    "    # Get list of sectors\n",
    "    sector_list = df_['Sector'].unique()\n",
    "\n",
    "    # Plot the percent price variation for each sector\n",
    "    for sector in sector_list:\n",
    "        \n",
    "        temp = df_[df_['Sector'] == sector]\n",
    "\n",
    "        plt.figure(figsize=(30,5))\n",
    "        plt.plot(temp['PRICE VAR [%]'])\n",
    "        plt.title(sector.upper())\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953b12f",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb212b2",
   "metadata": {},
   "source": [
    " copy paste from here https://www.kaggle.com/code/cnic92/explore-and-clean-financial-indicators-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(df):\n",
    "    # Get stocks that increased more than 500%\n",
    "    gain = 500\n",
    "    top_gainers = df[df['PRICE VAR [%]'] >= gain]\n",
    "    top_gainers = top_gainers['PRICE VAR [%]'].sort_values(ascending=False)\n",
    "    print(f'{len(top_gainers)} STOCKS with more than {gain}% gain.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6b0ee",
   "metadata": {},
   "source": [
    "## Outliers cleaning\n",
    "\n",
    "There are outliers/extreme values that are probably caused by mistypings. During our analysis of the data, we noticed that the values of NA and 0 were frequently used. We realized that 0 was used interchangeably with NA.  Also there are a lot of values that seem impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11119e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO!\n",
    "\"\"\"\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # IQR\n",
    "    test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "    for col in test:\n",
    "        iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "        print(iqr)\n",
    "\n",
    "    ## Z-Score: \n",
    "    threshold = 3\n",
    "\n",
    "    test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "    for col in test:\n",
    "        z_score= stats.zscore(df[col], nan_policy='omit')\n",
    "        outlier_indices = np.where(z_score > threshold)\n",
    "        print(outlier_indices)\n",
    "\n",
    "    # IQR\n",
    "    test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "    for col in test:\n",
    "        iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "        print(iqr)\n",
    "\n",
    "    #''''''\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faf4a8",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eac6a",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "There are a lot of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print(f'There are in total {df.isnull().sum().sum()} NAN in the dataframe')\n",
    "\n",
    "    ## Overview of all variables with missing values\n",
    "    df.isnull().mean().sort_values(ascending=False).plot.bar(figsize=(100,20))\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.title('Quantifying ALL missing data')\n",
    "    plt.show()\n",
    "\n",
    "    most_nan = df.isnull().mean().sort_values(ascending=False)\n",
    "    most_nan = most_nan[most_nan > 0.3]\n",
    "\n",
    "    most_nan.plot.bar(figsize=(20,20))\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.title('Data with more than 30% missing')\n",
    "    plt.show()\n",
    "\n",
    "    # Percentage of missing values for the variables\n",
    "    missing = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "    missing_data.head(25)\n",
    "\n",
    "    # Plot missing values 2.0\n",
    "    sns.heatmap(df.isna().transpose(), cmap=\"Blues\", cbar_kws={'label': 'Missing Values'});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2db1d",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df, threshold):\n",
    "\n",
    "    print(sum(df.isna().mean() > threshold)) # 76 of the remaining variables have more than 30% NAs\n",
    "    df = df.loc[::, df.isna().mean() < threshold] # drop all columns with NA proportion higher than threshold\n",
    "\n",
    "    numCols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(\"New numerical columns:\", numCols)\n",
    "    df[numCols] = df[numCols].fillna(df[numCols].median())\n",
    "\n",
    "    catCols = df.select_dtypes(exclude=np.number).columns\n",
    "    print(\"New categorical columns:\", catCols)\n",
    "    for col in catCols:\n",
    "        df[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    return numCols, catCols, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf05640",
   "metadata": {},
   "source": [
    "## Handling unique values and cardinality\n",
    "\n",
    "\n",
    "\"Each categorical variable consists of unique values. A categorical feature is said to possess high cardinality when there are too many of these unique values. One-Hot Encoding becomes a big problem in such a case since we have a separate column for each unique value (indicating its presence or absence) in the categorical variable. This leads to two problems, one is obviously space consumption, but this is not as big a problem as the second problem, the curse of dimensionality\" [reference here](https://towardsdatascience.com/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba0daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cardinality(column, threshold):\n",
    "    #threshold\n",
    "    threshold_value = int(threshold * len(column))\n",
    "    # Initialize\n",
    "    categories_list = []\n",
    "    s = 0\n",
    "    counts = []\n",
    "    \n",
    "    # Count the frequencies of unique values in the column\n",
    "    for value in column:\n",
    "        # Check if the value is already in the counts list\n",
    "        index = next((i for i, x in enumerate(counts) if x[0] == value), None)\n",
    "        if index is not None:\n",
    "            counts[index] = (value, counts[index][1] + 1)\n",
    "        else:\n",
    "            counts.append((value, 1))\n",
    "    \n",
    "    # Sort the list of tuples based on count in descending order\n",
    "    counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Loop through the tuples (value, count)\n",
    "    for i, j in counts:\n",
    "        # Add the frequency to the global sum\n",
    "        s += j\n",
    "        # Append the category name to the list\n",
    "        categories_list.append(i)\n",
    "        # Check if the global sum has reached the threshold value, if so break the loop\n",
    "        if s >= threshold_value:\n",
    "            break\n",
    "    \n",
    "    # Append the category 'Other' to the list\n",
    "    categories_list.append('Other')\n",
    "    \n",
    "    # Replace all instances not in our new categories by 'Other'\n",
    "    new_column=column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "    \n",
    "    return new_column, categories_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "#loading dataset\n",
    "df=load_dataset()\n",
    "\n",
    "#Call the function with a default threshold of 75%\n",
    "transformed_column,new_category_list=reduce_cardinality(df['Sector'], threshold=0.75)\n",
    "print(transformed_column)\n",
    "print(new_category_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e92f5",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ece60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummies(df, catCols):\n",
    "    df = pd.get_dummies(df, columns=catCols)\n",
    "    df.head()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cef1c2",
   "metadata": {},
   "source": [
    "## Get Most Significant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eaab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "def get_significant_features(X_train, X_test, y_train, n):\n",
    "    # Feature selection using Extra Trees Classifier on the resampled training data\n",
    "    model = ExtraTreesClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Select top features with highest importance scores\n",
    "    top_features = pd.Series(importances, index=X_train.columns).nlargest(n)\n",
    "\n",
    "    # Subset X_resampled and X_test with selected features\n",
    "    X_train_selected = X_train[top_features.index]\n",
    "    X_test_selected = X_test[top_features.index]\n",
    "\n",
    "    return X_train_selected, X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253531f8",
   "metadata": {},
   "source": [
    "## Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = load_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_duplicates_row(df)\n",
    "# check_duplicates_col(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47179304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "duplicated_columns = ['eBITperRevenue', 'eBTperEBIT', 'nIperEBT', 'Return on Tangible Assets', \n",
    "                     'ROIC', 'Payables Turnover', 'Inventory Turnover', 'Debt to Assets', 'Debt to Equity', \n",
    "                     'cashFlowCoverageRatios']\n",
    "df = remove_duplicates(df, duplicated_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "abs_corr_unstack = show_correlation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b67cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlation\n",
    "df = remove_correlation(df, abs_corr_unstack)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "# check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "numCols, catCols, df = handle_missing_data(df, 0.3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dummies\n",
    "df_dummies = add_dummies(df, catCols)\n",
    "df_dummies.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5772d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dummies[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "y = df['Class']\n",
    "\n",
    "## Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select most significant features\n",
    "X_train_selected, X_test_selected = get_significant_features(X_train, X_test, y_train, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StandardScaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Standardize features; equal results as if done in two\n",
    "X_train_std = sc.fit_transform(X_train_selected)\n",
    "X_test_std = sc.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for getting optimal C and gamma\n",
    "gamma_range = np.outer(np.logspace(-3, 0, 4),np.array([1,5]))\n",
    "gamma_range = gamma_range.flatten()\n",
    "print(gamma_range)\n",
    "\n",
    "C_range = np.outer(np.logspace(-1, 1, 3),np.array([1,5]))\n",
    "C_range = C_range.flatten()\n",
    "print(C_range)\n",
    "\n",
    "parameters = {'kernel':['linear', 'rbf'], 'C':C_range, 'gamma': gamma_range}\n",
    "\n",
    "svm = SVC()\n",
    "grid = RandomizedSearchCV(estimator=svm, param_distributions=parameters, n_iter=5, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train_std, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(grid.best_score_))\n",
    "print('Test score:       {:.2f}'.format(grid.score(X_test_std, y_test)))\n",
    "print('Best parameters: {}'.format(grid.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
