{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas_datareader import data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b155f9",
   "metadata": {},
   "source": [
    "## Loading the Data Set\n",
    "\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    project_directory = sys.path[0] ## get path of project directory\n",
    "    data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "    years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "    ## Loading the yearly dataset into the array dfs\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "        df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "        df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "        df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "        df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True) ## concat the diffrent dataframes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbd0e7",
   "metadata": {},
   "source": [
    "## Any Duplicates? \n",
    "\n",
    "No, there are no duplicates for rows but there are 20 duplicates for columns/ 10 each. Not same variable name but same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_row(df):\n",
    "    print(f'Duplicates in Rows:', True in list(df.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_col(df):\n",
    "    print(f'Duplicates in Columns:', True in list(df.T.duplicated().T))\n",
    "    print(\"Show the Duplicates:\")\n",
    "    print(df.T[df.T.duplicated(keep=False)].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df,columns):\n",
    "    shape_old=df.shape\n",
    "\n",
    "    df=df.drop(columns=columns)\n",
    "\n",
    "    print(f' Shape with duplicates:', shape_old) \n",
    "    print(f' Shape after removal of duplicates:', df.shape) \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0c759",
   "metadata": {},
   "source": [
    "## Correlation of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation(df):\n",
    "    X = df[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "    y = df[\"Class\"]\n",
    "    plt.matshow(X.corr().abs())\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    abs_corr = X.corr().abs()\n",
    "    for i in range(len(abs_corr)):\n",
    "        abs_corr.iloc[i, i] = 0\n",
    "        \n",
    "    abs_corr_unstack = abs_corr.unstack()\n",
    "    abs_corr_unstack.sort_values(kind=\"quicksort\")[-50:]\n",
    "\n",
    "    print((abs_corr_unstack.values>0.99).sum()/2)\n",
    "\n",
    "    return abs_corr_unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suggestion to deal with the correlations: remove a variable if its correlation with another variable is higher than 0.99\n",
    "def remove_correlation(df, abs_corr_unstack):\n",
    "    columns_to_drop = []\n",
    "    columns_to_remain = []\n",
    "\n",
    "    for pair in abs_corr_unstack.index.values:\n",
    "        if abs_corr_unstack[pair] > 0.99:\n",
    "            if pair[0] not in columns_to_remain and pair[1] not in columns_to_remain:\n",
    "                    columns_to_remain.append(pair[0])\n",
    "                    if pair[1] not in columns_to_drop:\n",
    "                        columns_to_drop.append(pair[1])\n",
    "            elif pair[0] in columns_to_remain:\n",
    "                if pair[1] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[1])\n",
    "            elif pair[1] in columns_to_remain:\n",
    "                if pair[0] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[0])\n",
    "\n",
    "    df_corr_removed = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df_corr_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49a06f",
   "metadata": {},
   "source": [
    "## Class Balance?\n",
    "\n",
    "The Variable `Class`is not balanced. We have to keep that in mind for train and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_imbalance(y):\n",
    "    sns.countplot(x=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169a0c0",
   "metadata": {},
   "source": [
    "## Outliers Dedection for `PRICE VAR[%]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39096800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_sectors(df):\n",
    "    df_ = df.loc[:, ['Sector','PRICE VAR [%]']]\n",
    "\n",
    "    # Get list of sectors\n",
    "    sector_list = df_['Sector'].unique()\n",
    "\n",
    "    # Plot the percent price variation for each sector\n",
    "    for sector in sector_list:\n",
    "        \n",
    "        temp = df_[df_['Sector'] == sector]\n",
    "\n",
    "        plt.figure(figsize=(30,5))\n",
    "        plt.plot(temp['PRICE VAR [%]'])\n",
    "        plt.title(sector.upper())\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953b12f",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb212b2",
   "metadata": {},
   "source": [
    " copy paste from here https://www.kaggle.com/code/cnic92/explore-and-clean-financial-indicators-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(df):\n",
    "    # Get stocks that increased more than 500%\n",
    "    gain = 500\n",
    "    top_gainers = df[df['PRICE VAR [%]'] >= gain]\n",
    "    top_gainers = top_gainers['PRICE VAR [%]'].sort_values(ascending=False)\n",
    "    print(f'{len(top_gainers)} STOCKS with more than {gain}% gain.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6b0ee",
   "metadata": {},
   "source": [
    "## Outliers cleaning\n",
    "\n",
    "There are outliers/extreme values that are probably caused by mistypings. During our analysis of the data, we noticed that the values of NA and 0 were frequently used. We realized that 0 was used interchangeably with NA.  Also there are a lot of values that seem impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11119e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO!\n",
    "\"\"\"\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # IQR\n",
    "    test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "    for col in test:\n",
    "        iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "        print(iqr)\n",
    "\n",
    "    ## Z-Score: \n",
    "    threshold = 3\n",
    "\n",
    "    test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "    for col in test:\n",
    "        z_score= stats.zscore(df[col], nan_policy='omit')\n",
    "        outlier_indices = np.where(z_score > threshold)\n",
    "        print(outlier_indices)\n",
    "\n",
    "    # IQR\n",
    "    test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "    for col in test:\n",
    "        iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "        print(iqr)\n",
    "\n",
    "    #''''''\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faf4a8",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eac6a",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "There are a lot of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print(f'There are in total {df.isnull().sum().sum()} NAN in the dataframe')\n",
    "\n",
    "    ## Overview of all variables with missing values\n",
    "    df.isnull().mean().sort_values(ascending=False).plot.bar(figsize=(100,20))\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.title('Quantifying ALL missing data')\n",
    "    plt.show()\n",
    "\n",
    "    most_nan = df.isnull().mean().sort_values(ascending=False)\n",
    "    most_nan = most_nan[most_nan > 0.3]\n",
    "\n",
    "    most_nan.plot.bar(figsize=(20,20))\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.title('Data with more than 30% missing')\n",
    "    plt.show()\n",
    "\n",
    "    # Percentage of missing values for the variables\n",
    "    missing = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "    missing_data.head(25)\n",
    "\n",
    "    # Plot missing values 2.0\n",
    "    sns.heatmap(df.isna().transpose(), cmap=\"Blues\", cbar_kws={'label': 'Missing Values'});\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fb2db1d",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df, threshold):\n",
    "\n",
    "    print(sum(df.isna().mean() > threshold)) # 76 of the remaining variables have more than 30% NAs\n",
    "    df = df.loc[::, df.isna().mean() < threshold] # drop all columns with NA proportion higher than threshold\n",
    "\n",
    "    numCols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(\"New numerical columns:\", numCols)\n",
    "    df[numCols] = df[numCols].fillna(df[numCols].median())\n",
    "\n",
    "    catCols = df.select_dtypes(exclude=np.number).columns\n",
    "    print(\"New categorical columns:\", catCols)\n",
    "    for col in catCols:\n",
    "        df[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    return numCols, catCols, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e92f5",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ece60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummies(df, catCols):\n",
    "    df = pd.get_dummies(df, columns=catCols)\n",
    "    df.head()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25cef1c2",
   "metadata": {},
   "source": [
    "## Get Most Significant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eaab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "def get_significant_features(X_train, X_test, y_train, n):\n",
    "    # Feature selection using Extra Trees Classifier on the resampled training data\n",
    "    model = ExtraTreesClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Select top features with highest importance scores\n",
    "    top_features = pd.Series(importances, index=X_train.columns).nlargest(n)\n",
    "\n",
    "    # Subset X_resampled and X_test with selected features\n",
    "    X_train_selected = X_train[top_features.index]\n",
    "    X_test_selected = X_test[top_features.index]\n",
    "\n",
    "    return X_train_selected, X_test_selected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "253531f8",
   "metadata": {},
   "source": [
    "## Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = load_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_duplicates_row(df)\n",
    "# check_duplicates_col(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47179304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns\n",
    "duplicated_columns = ['eBITperRevenue', 'eBTperEBIT', 'nIperEBT', 'Return on Tangible Assets', \n",
    "                     'ROIC', 'Payables Turnover', 'Inventory Turnover', 'Debt to Assets', 'Debt to Equity', \n",
    "                     'cashFlowCoverageRatios']\n",
    "df = remove_duplicates(df, duplicated_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation\n",
    "abs_corr_unstack = show_correlation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b67cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove correlation\n",
    "df = remove_correlation(df, abs_corr_unstack)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "# check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "numCols, catCols, df = handle_missing_data(df, 0.3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dummies\n",
    "df_dummies = add_dummies(df, catCols)\n",
    "df_dummies.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5772d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dummies[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "y = df['Class']\n",
    "\n",
    "## Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select most significant features\n",
    "X_train_selected, X_test_selected = get_significant_features(X_train, X_test, y_train, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StandardScaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Standardize features; equal results as if done in two\n",
    "X_train_std = sc.fit_transform(X_train_selected)\n",
    "X_test_std = sc.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for getting optimal C and gamma\n",
    "gamma_range = np.outer(np.logspace(-3, 0, 4),np.array([1,5]))\n",
    "gamma_range = gamma_range.flatten()\n",
    "print(gamma_range)\n",
    "\n",
    "C_range = np.outer(np.logspace(-1, 1, 3),np.array([1,5]))\n",
    "C_range = C_range.flatten()\n",
    "print(C_range)\n",
    "\n",
    "parameters = {'kernel':['linear', 'rbf'], 'C':C_range, 'gamma': gamma_range}\n",
    "\n",
    "svm = SVC()\n",
    "grid = RandomizedSearchCV(estimator=svm, param_distributions=parameters, n_iter=5, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train_std, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(grid.best_score_))\n",
    "print('Test score:       {:.2f}'.format(grid.score(X_test_std, y_test)))\n",
    "print('Best parameters: {}'.format(grid.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSLab",
   "language": "python",
   "name": "cslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
