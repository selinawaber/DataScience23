{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12441df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b155f9",
   "metadata": {},
   "source": [
    "## Loading the Data Set\n",
    "\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = sys.path[0] ## get path of project directory\n",
    "data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "## Loading the yearly dataset into the array dfs\n",
    "dfs = []\n",
    "for year in years:\n",
    "    df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "    df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "    df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "    df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "    df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "    dfs.append(df)\n",
    "    \n",
    "    \n",
    "df = pd.concat(dfs) ## concat the diffrent dataframes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde251e1",
   "metadata": {},
   "source": [
    "## Some Explanation of Variables:\n",
    "\n",
    "### Adding  `year` as a cathegorical variable\n",
    "\n",
    "We added a column named `year` which contains the respecitve year.\n",
    "\n",
    "\n",
    "\n",
    "### Handling the variable `Price VAR [%]`\n",
    "\n",
    "The last column, `PRICE VAR [%]`, lists the percent price variation of each stock for the year. For example, if we consider the dataset 2015_Financial_Data.csv, we will have:\n",
    "\n",
    "- 200+ financial indicators for the year 2015;\n",
    "- percent price variation for the year 2016 (meaning from the first trading day on Jan 2016 to the last trading day on Dec 2016).\n",
    "\n",
    "We renamed all the variables with the specific year in it, e.g.  `2016 PRICE VAR [%]` to `PRICE VAR [%]`. We dropped the old ones.Now we just have one variable `PRICE VAR [%]`. \n",
    "\n",
    "\n",
    "### The variable `class`\n",
    "\n",
    "class lists a binary classification for each stock, where\n",
    "\n",
    "- for each stock, if the PRICE VAR [%] value is positive, class = 1. From a trading perspective, the 1 identifies those stocks that an hypothetical trader should BUY at the start of the year and sell at the end of the year for a profit.\n",
    "- for each stock, if the PRICE VAR [%] value is negative, class = 0. From a trading perspective, the 0 identifies those stocks that an hypothetical trader should NOT BUY, since their value will decrease, meaning a loss of capital.\n",
    "\n",
    "\n",
    "The columns `PRICE VAR [%]` and `class` make possible to use the datasets for both classification and regression tasks:\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to classify those stocks that in buy-worthy and not buy-worthy, it is possible to get the targets from the class column;\n",
    "- If the user wishes to train a machine learning model so that it learns to predict the future value of a stock, it is possible to get the targets from the PRICE VAR [%] column.\n",
    "\n",
    "\n",
    "### The variable  `Stock Name`\n",
    "\n",
    "We named the first variable `Stock Name`since it has not been named in the original dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403b8fc",
   "metadata": {},
   "source": [
    "## First Description of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a19739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001949c",
   "metadata": {},
   "source": [
    "## Numerical and Catgorical Features/Variables\n",
    "\n",
    "We are converting `Class`to a cathegorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'] = df.Class.astype('object') ## object or catheogry?? whats the difference???\n",
    "\n",
    "numCols = list(df.select_dtypes(exclude='object').columns)\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\")\n",
    "\n",
    "catCols = list(df.select_dtypes(include='object').columns)\n",
    "print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d8e96",
   "metadata": {},
   "source": [
    "## Any Duplicates? \n",
    "\n",
    "No, there are no duplicates for rows but there are 20 duplicates for columns/ 10 each. Not same variable name but same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc87526",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in Rows:', True in list(df.duplicated()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b449146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in Columns:', True in list(df.T.duplicated().T))\n",
    "print(\"Show the Duplicates:\")\n",
    "print(df.T[df.T.duplicated(keep=False)].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60958a0",
   "metadata": {},
   "source": [
    "Our Duplicates are the following pairs:\n",
    "\n",
    "- `ebitperRevenue` and `eBITperRevenu`\n",
    "- `ebtperEBIT` and `eBTperEBIT`\n",
    "- `niperEBT` and `nIperEBT`\n",
    "- `returnOnAssets` and `Return on Tangible Assets`\n",
    "- `returnOnCapitalEmployed` and `ROIC`\n",
    "- `payablesTurnover` and `Payables Turnover`\n",
    "- `inventoryTurnover` and `Inventory Turnover`\n",
    "- `debtRatio` and `Debt to Assets`\n",
    "- `debtEquityRatio` and `Debt to Equity`\n",
    "- `cashFlowToDebtRatio` and `cashFlowCoverageRatios`\n",
    "\n",
    "We will remove the first occurence of the duplicates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_old=df.shape\n",
    "df= df.T.drop_duplicates().T # remove duplicates!\n",
    "print(f' Shape with duplicates:', shape_old) \n",
    "print(f' Shape after removal of duplicates:', df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b22d9",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "There are a lot of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are in total {df.isnull().sum().sum()} NAN in the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#df.replace(to_replace = 0, value = np.nan, inplace=true) ????? DA EVTL noch mehr mit NAN ersetzen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of all variables with missing values\n",
    "df.isnull().mean().sort_values(ascending=False).plot.bar(figsize=(100,20))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Quantifying ALL missing data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_nan = df.isnull().mean().sort_values(ascending=False)\n",
    "most_nan = most_nan[most_nan > 0.3]\n",
    "\n",
    "most_nan.plot.bar(figsize=(20,20))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Data with more than 30% missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe386a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "missing_data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ea8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values 2.0\n",
    "sns.heatmap(df.isna().transpose(), cmap=\"Blues\", cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4b92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c5f8016",
   "metadata": {},
   "source": [
    "## Outliers cleaning\n",
    "\n",
    "There are outliers/extreme values that are probably caused by mistypings. During our analysis of the data, we noticed that the values of NA and 0 were frequently used. We realized that 0 was used interchangeably with NA.  Also there are a lot of values that seem impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z-Score: \n",
    "\n",
    "threshold = 3\n",
    "\n",
    "test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "for col in test:\n",
    "    z_score= stats.zscore(df[col], nan_policy='omit')\n",
    "    outlier_indices = np.where(z_score > threshold)\n",
    "    print(outlier_indices )\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0ad68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR\n",
    "test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "for col in test:\n",
    "    iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "    print(iqr)\n",
    "\n",
    "#''''''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75de484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
