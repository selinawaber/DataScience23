{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas_datareader import data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37b155f9",
   "metadata": {},
   "source": [
    "## Loading the Data Set\n",
    "\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = sys.path[0] ## get path of project directory\n",
    "data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "## Loading the yearly dataset into the array dfs\n",
    "dfs = []\n",
    "for year in years:\n",
    "    df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "    df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "    df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "    df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "    df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "    dfs.append(df)\n",
    "    \n",
    "    \n",
    "df = pd.concat(dfs, ignore_index=True) ## concat the diffrent dataframes\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c9d5a87",
   "metadata": {},
   "source": [
    "## Some Explanation of Variables:\n",
    "\n",
    "### Adding  `year` as a cathegorical variable\n",
    "\n",
    "We added a column named `year` which contains the respecitve year.\n",
    "\n",
    "\n",
    "\n",
    "### Handling the variable `Price VAR [%]`\n",
    "\n",
    "The last column, `PRICE VAR [%]`, lists the percent price variation of each stock for the year. For example, if we consider the dataset 2015_Financial_Data.csv, we will have:\n",
    "\n",
    "- 200+ financial indicators for the year 2015;\n",
    "- percent price variation for the year 2016 (meaning from the first trading day on Jan 2016 to the last trading day on Dec 2016).\n",
    "\n",
    "We renamed all the variables with the specific year in it, e.g.  `2016 PRICE VAR [%]` to `PRICE VAR [%]`. We dropped the old ones.Now we just have one variable `PRICE VAR [%]`. \n",
    "\n",
    "\n",
    "### The variable `class`\n",
    "\n",
    "class lists a binary classification for each stock, where\n",
    "\n",
    "- for each stock, if the PRICE VAR [%] value is positive, class = 1. From a trading perspective, the 1 identifies those stocks that an hypothetical trader should BUY at the start of the year and sell at the end of the year for a profit.\n",
    "- for each stock, if the PRICE VAR [%] value is negative, class = 0. From a trading perspective, the 0 identifies those stocks that an hypothetical trader should NOT BUY, since their value will decrease, meaning a loss of capital.\n",
    "\n",
    "\n",
    "The columns `PRICE VAR [%]` and `class` make possible to use the datasets for both classification and regression tasks:\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to classify those stocks that in buy-worthy and not buy-worthy, it is possible to get the targets from the class column;\n",
    "- If the user wishes to train a machine learning model so that it learns to predict the future value of a stock, it is possible to get the targets from the PRICE VAR [%] column.\n",
    "\n",
    "\n",
    "### The variable  `Stock Name`\n",
    "\n",
    "We named the first variable `Stock Name`since it has not been named in the original dataset.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ec6629f",
   "metadata": {},
   "source": [
    "## First Description of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6fd87fb",
   "metadata": {},
   "source": [
    "## Numerical and Catgorical Features/Variables\n",
    "\n",
    "We are converting `Class`to a cathegorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'] = df.Class.astype('object') ## object or catheogry?? whats the difference???\n",
    "\n",
    "numCols = list(df.select_dtypes(exclude='object').columns)\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\")\n",
    "\n",
    "catCols = list(df.select_dtypes(include='object').columns)\n",
    "print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1fbd0e7",
   "metadata": {},
   "source": [
    "## Any Duplicates? \n",
    "\n",
    "No, there are no duplicates for rows but there are 20 duplicates for columns/ 10 each. Not same variable name but same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in Rows:', True in list(df.duplicated()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in Columns:', True in list(df.T.duplicated().T))\n",
    "print(\"Show the Duplicates:\")\n",
    "print(df.T[df.T.duplicated(keep=False)].T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf38ff15",
   "metadata": {},
   "source": [
    "Our Duplicates are the following pairs:\n",
    "\n",
    "- `ebitperRevenue` and `eBITperRevenu`\n",
    "- `ebtperEBIT` and `eBTperEBIT`\n",
    "- `niperEBT` and `nIperEBT`\n",
    "- `returnOnAssets` and `Return on Tangible Assets`\n",
    "- `returnOnCapitalEmployed` and `ROIC`\n",
    "- `payablesTurnover` and `Payables Turnover`\n",
    "- `inventoryTurnover` and `Inventory Turnover`\n",
    "- `debtRatio` and `Debt to Assets`\n",
    "- `debtEquityRatio` and `Debt to Equity`\n",
    "- `cashFlowToDebtRatio` and `cashFlowCoverageRatios`\n",
    "\n",
    "We will remove the first occurence of the duplicates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_old=df.shape\n",
    "\n",
    "## HUGHE PROBLEM --> df.drop_duplicates() transforms all variables to the datatype 'object'!\n",
    "#I don't know why it does that!\n",
    "# So i will remove the manually!!!\n",
    "\n",
    "#df= df.T.drop_duplicates().T # remove duplicates!\n",
    "\n",
    "df= df.drop(columns=['eBITperRevenue', 'eBTperEBIT', 'nIperEBT', 'Return on Tangible Assets', \n",
    "                     'ROIC', 'Payables Turnover', 'Inventory Turnover', 'Debt to Assets', 'Debt to Equity', \n",
    "                     'cashFlowCoverageRatios'])\n",
    "\n",
    "print(f' Shape with duplicates:', shape_old) \n",
    "print(f' Shape after removal of duplicates:', df.shape) \n",
    "\n",
    "\n",
    "#print(df.info(verbose=True)) ok good, sill folat64 objects\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f49a06f",
   "metadata": {},
   "source": [
    "## Class Balance?\n",
    "\n",
    "The Variable `Class`is not balanced. We have to keep that in mind for train and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target Variable\n",
    "#sns.barplot(x = '0', y = '1', data = df['Class'].value_counts())\n",
    "#plt.show()\n",
    "\n",
    "#That won't work??? WHY????"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b246fd3",
   "metadata": {},
   "source": [
    "## Variation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e31255a",
   "metadata": {},
   "source": [
    "There is something seriously wrong with the variable `operatingProfitMargin`. It has a Standard Deviation of 0, which cant be! So we dropp it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72084a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "no_std= [ std for std in df.std( axis=0, skipna=True, numeric_only=True) if std <= 0.15]\n",
    "print(no_std)\n",
    "\n",
    "\n",
    "\n",
    "#x = df.std( axis = 0, skipna = True, numeric_only = True)\n",
    "#[(index, x[index]) for index in x.index.values if x[index] > 0]\n",
    "\n",
    "### TO DOOO!!!!!!!!!!!!!!!!!\n",
    "\n",
    "df= df.drop(columns= ['operatingProfitMargin'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4169a0c0",
   "metadata": {},
   "source": [
    "## Outliers Dedection for `PRICE VAR[%]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39096800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.loc[:, ['Sector','PRICE VAR [%]']]\n",
    "\n",
    "# Get list of sectors\n",
    "sector_list = df_['Sector'].unique()\n",
    "\n",
    "# Plot the percent price variation for each sector\n",
    "for sector in sector_list:\n",
    "    \n",
    "    temp = df_[df_['Sector'] == sector]\n",
    "\n",
    "    plt.figure(figsize=(30,5))\n",
    "    plt.plot(temp['PRICE VAR [%]'])\n",
    "    plt.title(sector.upper())\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a953b12f",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bb212b2",
   "metadata": {},
   "source": [
    " copy paste from here https://www.kaggle.com/code/cnic92/explore-and-clean-financial-indicators-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stocks that increased more than 500%\n",
    "gain = 500\n",
    "top_gainers = df[df['PRICE VAR [%]'] >= gain]\n",
    "top_gainers = top_gainers['PRICE VAR [%]'].sort_values(ascending=False)\n",
    "print(f'{len(top_gainers)} STOCKS with more than {gain}% gain.')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would be cool if that would work.........\n",
    "\n",
    "# Set\n",
    "date_start = '01-01-2015'\n",
    "date_end = '12-31-2015'\n",
    "tickers = top_gainers.index.values.tolist()\n",
    "\n",
    "for ticker in tickers:\n",
    "    \n",
    "    # Pull daily prices for each ticker from Yahoo Finance\n",
    "    daily_price = data.DataReader(ticker, 'yahoo', date_start, date_end)\n",
    "    \n",
    "    # Plot prices with volume\n",
    "    fig, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    ax0.plot(daily_price['Adj Close'])\n",
    "    ax0.set_title(ticker, fontsize=18)\n",
    "    ax0.set_ylabel('Daily Adj Close $', fontsize=14)\n",
    "    ax1.plot(daily_price['Volume'])\n",
    "    ax1.set_ylabel('Volume', fontsize=14)\n",
    "    ax1.yaxis.set_major_formatter(\n",
    "            matplotlib.ticker.StrMethodFormatter('{x:.0E}'))\n",
    "\n",
    "    fig.align_ylabels(ax1)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fc6b0ee",
   "metadata": {},
   "source": [
    "## Outliers cleaning\n",
    "\n",
    "There are outliers/extreme values that are probably caused by mistypings. During our analysis of the data, we noticed that the values of NA and 0 were frequently used. We realized that 0 was used interchangeably with NA.  Also there are a lot of values that seem impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11119e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR\n",
    "test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "for col in test:\n",
    "    iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "    print(iqr)\n",
    "\n",
    "#''''''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77faf4a8",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a66eac6a",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "There are a lot of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are in total {df.isnull().sum().sum()} NAN in the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11afbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.replace([np.inf, -np.inf], np.nan, inplace=True) do that in Outliers Section!\n",
    "#df.replace(to_replace = 0, value = np.nan, inplace=true) ????? DA EVTL noch mehr mit NAN ersetzen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of all variables with missing values\n",
    "df.isnull().mean().sort_values(ascending=False).plot.bar(figsize=(100,20))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Quantifying ALL missing data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a32640",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_nan = df.isnull().mean().sort_values(ascending=False)\n",
    "most_nan = most_nan[most_nan > 0.3]\n",
    "\n",
    "most_nan.plot.bar(figsize=(20,20))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Data with more than 30% missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "missing_data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values 2.0\n",
    "sns.heatmap(df.isna().transpose(), cmap=\"Blues\", cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86f856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a85406b",
   "metadata": {},
   "source": [
    "## Assigning X and target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db9e92f5",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b031cee",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47596a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "699a8a80",
   "metadata": {},
   "source": [
    "## Important Link\n",
    "\n",
    "[Cleaning Data Set](https://www.kaggle.com/code/cnic92/explore-and-clean-financial-indicators-dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13dac58b",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels=[\"Class\", \"Stock Name\", \"Sector\", \"year\", \"PRICE VAR [%]\"], axis = 1)\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71996d6a",
   "metadata": {},
   "source": [
    "### Tentative procedure for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all columns with nr. of NaN > 30%\n",
    "\n",
    "print(\"Shape of X data:\", X.shape)\n",
    "\n",
    "most_nan_columns = X.isnull().mean(axis = 0)\n",
    "most_nan_columns = most_nan_columns[most_nan_columns > 0.3]\n",
    "\n",
    "X = X.drop(columns=most_nan_columns.index.values)\n",
    "\n",
    "print(\"Shape of X data after removing columns with nr. of NaN > 30%:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e59a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all rows with nr. of NaN > 30%\n",
    "\n",
    "most_nan_rows = X.isnull().mean(axis = 1)\n",
    "most_nan_rows = most_nan_rows[most_nan_rows > 0.3]\n",
    "\n",
    "X = X.drop(labels=most_nan_rows.index.values, axis = 0)\n",
    "y = y.drop(labels=most_nan_rows.index.values, axis = 0)\n",
    "\n",
    "print(\"Shape of X, y data after removing rows with nr. of NaN > 30%:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ec08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all remaining NaN with mean value (this technique is explicitely not recommended)\n",
    "\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of the data\n",
    "\n",
    "X = (X-X.mean())/X.std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e310adef",
   "metadata": {},
   "source": [
    "### FCNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c277ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3, input_dim = 192):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, input_dim//2)\n",
    "        self.lin2 = nn.Linear(input_dim//2, input_dim//4)\n",
    "        self.lin3 = nn.Linear(input_dim//4, 1)\n",
    "        self.drop1 = nn.Dropout(p = dropout_rate)\n",
    "        self.drop2 = nn.Dropout(p = dropout_rate)\n",
    "        self.prob = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.prob(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de166c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0938f7ff",
   "metadata": {},
   "source": [
    "### Grid Search for optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation of the data to tensors for the Grid Search\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.astype(\"int32\").values, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"optimizer__lr\": [0.001, 0.005, 0.01], \n",
    "              \"optimizer__momentum\": [0.6, 0.8, 0.9],\n",
    "              \"module__dropout_rate\": [0.2, 0.3, 0.4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67823cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetClassifier(\n",
    "    FCNetwork,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.SGD,\n",
    "    max_epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ca76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs = -1)\n",
    "grid_result = grid.fit(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0661df89",
   "metadata": {},
   "source": [
    "### Training the Neural Network with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lr = grid_result.best_params_[\"optimizer__lr\"]\n",
    "optimal_momentum = grid_result.best_params_[\"optimizer__momentum\"]\n",
    "optimal_dropout = grid_result.best_params_[\"module__dropout_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of Dataset for Dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.astype(\"int32\").values, dtype=torch.float32).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of train-, evaluation- and test-set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the Dataloader\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc59c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FCNetwork(input_dim = input_dim, dropout_rate = optimal_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr=optimal_lr, momentum=optimal_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = float(\"inf\")\n",
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    network.train()\n",
    "    n = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "    \n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / n:.3f}')\n",
    "\n",
    "    network.eval()\n",
    "    val_loss = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            n += 1\n",
    "            \n",
    "            \n",
    "            #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                #print(f'[{epoch + 1}, {i + 1:5d}] loss: {val_loss / 2000:.3f}')\n",
    "                #val_loss = 0.0\n",
    "    \n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {val_loss / n:.3f}')            \n",
    "                \n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        print(f\"The new best model is at epoch {epoch}\")\n",
    "        torch.save(network.state_dict(), PATH)\n",
    "    print(f'Epoch: {epoch} over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FCNetwork()\n",
    "network.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85026761",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "network.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = network(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = outputs>0.5 \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37295916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d0b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
