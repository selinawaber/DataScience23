{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas_datareader import data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b155f9",
   "metadata": {},
   "source": [
    "## Loading the Data Set\n",
    "\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    project_directory = sys.path[0] ## get path of project directory\n",
    "    data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "    years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "    ## Loading the yearly dataset into the array dfs\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "        df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "        df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "        df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "        df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "        dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(dfs, ignore_index=True) ## concat the diffrent dataframes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174a163",
   "metadata": {},
   "source": [
    "## Some Explanation of Variables:\n",
    "\n",
    "### Adding `year` as a categorical variable\n",
    "\n",
    "We added a column named year which contains the respecitve year.\n",
    "\n",
    "### Handling the variable `Price VAR [%]`\n",
    "\n",
    "The last column, `PRICE VAR [%]`, lists the percent price variation of each stock for the year. For example, if we consider the dataset 2015_Financial_Data.csv, we will have:\n",
    "\n",
    "- 200+ financial indicators for the year 2015;\n",
    "- percent price variation for the year 2016 (meaning from the first trading day on Jan 2016 to the last trading day on Dec 2016).\n",
    "\n",
    "We renamed all the variables with the specific year in it, e.g. `2016 PRICE VAR [%]` to `PRICE VAR [%]`. We dropped the old ones. \n",
    "\n",
    "### the variable `class`\n",
    "\n",
    "class lists a binary classification for each stock, where\n",
    "\n",
    "- for each stock, if the PRICE VAR [%] value is positive, class = 1. From a trading perspective, the 1 identifies those stocks that an hypothetical trader should BUY at the start of the year and sell at the end of the year for a profit.\n",
    "- for each stock, if the PRICE VAR [%] value is negative, class = 0. From a trading perspective, the 0 identifies those stocks that an hypothetical trader should NOT BUY, since their value will decrease, meaning a loss of capital.\n",
    "\n",
    "The columns `PRICE VAR [%]` and `class` make possible to use the datasets for both classification and regression tasks:\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to classify those stocks that in buy-worthy and not buy-worthy, it is possible to get the targets from the class column;\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to predict the future value of a stock, it is possible to get the targets from the PRICE VAR [%] column.\n",
    "\n",
    "### the variable `Stock Name`\n",
    "\n",
    "We named the first variable Stock Namesince it has not been named in the original dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba5839",
   "metadata": {},
   "source": [
    "## Numerical and Catgorical Features/Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are converting Classto a cathegorical variable.\n",
    "def class_to_categorical(df):\n",
    "    df['Class'] = df.Class.astype('object') ## object or catheogry?? whats the difference??\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04fd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_numerical_categorical_variables(df):\n",
    "    \n",
    "    numCols = df.select_dtypes(exclude='object').columns\n",
    "    print(f\"There are {len(numCols)} numerical features:\\n\")\n",
    "\n",
    "    catCols = df.select_dtypes(include='object').columns\n",
    "    print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbd0e7",
   "metadata": {},
   "source": [
    "## Any Duplicates? \n",
    "\n",
    "No, there are no duplicates for rows but there are 20 duplicates for columns/ 10 each. Not same variable name but same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_row(df):\n",
    "    print(f'Duplicates in Rows:', True in list(df.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_col(df):\n",
    "    print(f'Duplicates in Columns:', True in list(df.T.duplicated().T))\n",
    "    print(\"Show the Duplicates:\")\n",
    "    print(df.T[df.T.duplicated(keep=False)].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df,columns):\n",
    "    shape_old=df.shape\n",
    "\n",
    "    df=df.drop(columns=columns)\n",
    "\n",
    "    #print(f' Shape with duplicates:', shape_old) \n",
    "    #print(f' Shape after removal of duplicates:', df.shape) \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c0467",
   "metadata": {},
   "source": [
    "Our Duplicates are the following pairs:\n",
    "\n",
    "- `ebitperRevenue` and `eBITperRevenu`\n",
    "- `ebtperEBIT` and `eBTperEBIT`\n",
    "- `niperEBT` and `nIperEBT`\n",
    "- `returnOnAssets` and `Return on Tangible Assets`\n",
    "- `returnOnCapitalEmployed` and `ROIC`\n",
    "- `payablesTurnover` and `Payables Turnover`\n",
    "- `inventoryTurnover` and `Inventory Turnover`\n",
    "- `debtRatio` and `Debt to Assets`\n",
    "- `debtEquityRatio` and `Debt to Equity`\n",
    "- `cashFlowToDebtRatio` and `cashFlowCoverageRatios`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1d817",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a4854",
   "metadata": {},
   "source": [
    "We got the data from the following webpages: [S&P means](https://www.macrotrends.net/2526/sp-500-historical-annual-returns) and [inflation](https://www.macrotrends.net/countries/USA/united-states/inflation-rate-cpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2515b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_indicators(df):\n",
    "    \n",
    "    ## Yearly Means of S&P 500\n",
    "    sp500_means = pd.Series([11.39, -0.73, 9.54, 19.42, -6.24], index = [2014, 2015, 2016, 2017, 2018]) ## or should it start with year 2015 to year 2019???\n",
    "    ## for year 2019 we got 28.88%\n",
    "    \n",
    "    # Yearly Inflaction Rate measured by consumer price index\n",
    "    inflation = pd.Series([1.62, 0.12, 1.26, 2.13, 2.44], index = [2014, 2015, 2016, 2017, 2018]) ## or should it start with year 2015 to year 2019???\n",
    "    ## or should we look at annual change????\n",
    "    \n",
    "    ##Adding to the dataframe\n",
    "    df[\"inflation\"] = df.apply(lambda x: inflation[int(x[\"year\"])], axis=1)\n",
    "    df[\"sp500_means\"] = df.apply(lambda x: sp500_means[int(x[\"year\"])], axis=1)\n",
    "    \n",
    "    \n",
    "    ## Calculation of Excess Return:\n",
    "    df[\"excess_return\"] = np.subtract(df[\"PRICE VAR [%]\"], df[\"sp500_means\"])\n",
    "\n",
    "\n",
    "    ## Calculation of Cashflow Margin:\n",
    "    df[\"cashflow_margin\"] = df[\"Operating Cash Flow\"].divide(df[\"Revenue\"])\n",
    "    # Pay attention to ZeroDivisionError, replace infinity by NAN\n",
    "    df[\"cashflow_margin\"] = df[\"cashflow_margin\"].replace([np.inf, -np.inf], np.nan)\n",
    "   \n",
    "    \n",
    "    ## Calculation of Return on Net Assets (RONA)\n",
    "    df[\"Net_working_capital\"] = df[\"Total assets\"]-df[\"Cash and cash equivalents\"]\n",
    "    df[\"RONA\"] = df[\"EBIT\"]/df[\"Net_working_capital\"]\n",
    "    df[\"RONA\"] = df[\"RONA\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "    #df[\"operating_liabilities\"] = df[\"Total liabilities\"]-df[\"Total debt\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29db63d",
   "metadata": {},
   "source": [
    "### Dropping obviously wrong indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_indicators(df):\n",
    "    df_new = df.drop([\"operatingProfitMargin\"], axis = 1) # consisting only of the value 1.\n",
    "    \n",
    "    #maybe more to drop? which are not yet addressed in correlation or duplicates or elsewhere?\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0c759",
   "metadata": {},
   "source": [
    "## Correlation of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correlation(df):\n",
    "    X = df[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "    y = df[\"Class\"]\n",
    "    plt.matshow(X.corr().abs())\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd14ee",
   "metadata": {},
   "source": [
    " --> double check if abs_corr_unstack is correct!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df):\n",
    "    X = df[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "    y = df[\"Class\"]\n",
    "\n",
    "    abs_corr = X.corr().abs()\n",
    "    for i in range(len(abs_corr)):\n",
    "        abs_corr.iloc[i, i] = 0\n",
    "        \n",
    "    abs_corr_unstack = abs_corr.unstack()\n",
    "    abs_corr_unstack.sort_values(kind=\"quicksort\")[-50:] # Why that?\n",
    "\n",
    "    #print((abs_corr_unstack.values>0.99).sum()/2)\n",
    "\n",
    "    return abs_corr_unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suggestion to deal with the correlations: remove a variable if its correlation with another variable is higher than 0.99\n",
    "def remove_correlation(df, abs_corr_unstack):\n",
    "    columns_to_drop = []\n",
    "    columns_to_remain = []\n",
    "\n",
    "    for pair in abs_corr_unstack.index.values:\n",
    "        if abs_corr_unstack[pair] > 0.99:\n",
    "            if pair[0] not in columns_to_remain and pair[1] not in columns_to_remain:\n",
    "                    columns_to_remain.append(pair[0])\n",
    "                    if pair[1] not in columns_to_drop:\n",
    "                        columns_to_drop.append(pair[1])\n",
    "            elif pair[0] in columns_to_remain:\n",
    "                if pair[1] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[1])\n",
    "            elif pair[1] in columns_to_remain:\n",
    "                if pair[0] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[0])\n",
    "\n",
    "    df_corr_removed = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    return df_corr_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49a06f",
   "metadata": {},
   "source": [
    "## Class Balance?\n",
    "\n",
    "The Variable `Class`is not balanced. We have to keep that in mind for train and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_imbalance(y):\n",
    "    sns.countplot(x=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169a0c0",
   "metadata": {},
   "source": [
    "## Outliers Dedection for `PRICE VAR[%]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39096800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_sectors(df):\n",
    "    df_ = df.loc[:, ['Sector','PRICE VAR [%]']]\n",
    "\n",
    "    # Get list of sectors\n",
    "    sector_list = df_['Sector'].unique()\n",
    "\n",
    "    # Plot the percent price variation for each sector\n",
    "    for sector in sector_list:\n",
    "        \n",
    "        temp = df_[df_['Sector'] == sector]\n",
    "\n",
    "        plt.figure(figsize=(30,5))\n",
    "        plt.plot(temp['PRICE VAR [%]'])\n",
    "        plt.title(sector.upper())\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953b12f",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb212b2",
   "metadata": {},
   "source": [
    " copy paste from here https://www.kaggle.com/code/cnic92/explore-and-clean-financial-indicators-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(df):\n",
    "    # Get stocks that increased more than 500%\n",
    "    gain = 500\n",
    "    top_gainers = df[df['PRICE VAR [%]'] >= gain]\n",
    "    top_gainers = top_gainers['PRICE VAR [%]'].sort_values(ascending=False)\n",
    "    print(f'{len(top_gainers)} STOCKS with more than {gain}% gain.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6b0ee",
   "metadata": {},
   "source": [
    "## Outliers cleaning\n",
    "\n",
    "There are outliers/extreme values that are probably caused by mistypings. During our analysis of the data, we noticed that the values of NA and 0 were frequently used. We realized that 0 was used interchangeably with NA.  Also there are a lot of values that seem impossible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11119e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def remove_outliers(X_train, X_test, y_train, y_test):\n",
    "    ## Isolation Forest\n",
    "    outliers = IsolationForest(random_state = 42).fit(X_train) # fit Isolation Forest only to training data\n",
    "    outliers_train = outliers.predict(X_train)\n",
    "    outliers_test = outliers.predict(X_test)\n",
    "\n",
    "    ## Remove outliers where 1 represent inliers and -1 represent outliers:\n",
    "    X_train_cleaned = X_train[np.where(outliers_train == 1, True, False)]\n",
    "    y_train_cleaned = y_train[np.where(outliers_train == 1, True, False)]\n",
    "    X_test_cleaned = X_test[np.where(outliers_test == 1, True, False)]\n",
    "    y_test_cleaned = y_test[np.where(outliers_test == 1, True, False)]\n",
    "    print(\"Shape with outliers: \", X_train.shape,\", Shape without outliers: \", X_train_cleaned.shape,\", Removed outliers: \", X_train.shape[0]-X_train_cleaned.shape[0])\n",
    "    print(\"Shape with outliers: \", X_test.shape,\", Shape without outliers: \", X_test_cleaned.shape,\", Removed outliers: \", X_test.shape[0]-X_test_cleaned.shape[0]) \n",
    "    \n",
    "\n",
    "    return X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faf4a8",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eac6a",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "There are a lot of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print(f'There are in total {df.isnull().sum().sum()} NAN in the dataframe')\n",
    "\n",
    "    ## Overview of all variables with missing values\n",
    "    df.isnull().mean().sort_values(ascending=False).plot.bar(figsize=(100,20))\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.title('Quantifying ALL missing data')\n",
    "    plt.show()\n",
    "\n",
    "    most_nan = df.isnull().mean().sort_values(ascending=False)\n",
    "    most_nan = most_nan[most_nan > 0.3]\n",
    "\n",
    "    most_nan.plot.bar(figsize=(20,20))\n",
    "    plt.ylabel('Percentage of missing values')\n",
    "    plt.xlabel('Variables')\n",
    "    plt.title('Data with more than 30% missing')\n",
    "    plt.show()\n",
    "\n",
    "    # Percentage of missing values for the variables\n",
    "    missing = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "    missing_data.head(25)\n",
    "\n",
    "    # Plot missing values 2.0\n",
    "    sns.heatmap(df.isna().transpose(), cmap=\"Blues\", cbar_kws={'label': 'Missing Values'});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2db1d",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df, threshold):\n",
    "\n",
    "    print(sum(df.isna().mean() > threshold)) # 76 of the remaining variables have more than 30% NAs\n",
    "    # Calculate the percentage of missing values in each column\n",
    "    missing_values_percentage = df.isna().sum() / df.shape[0]\n",
    "    # Identify the columns with a higher percentage of missing values than the threshold\n",
    "    columns_to_drop = missing_values_percentage[missing_values_percentage > threshold].index\n",
    "    print(f\"Columns to drop: {columns_to_drop}\")\n",
    "    \n",
    "    # df = df.loc[::, df.isna().mean() < threshold] # drop all columns with NA proportion higher than threshold\n",
    "\n",
    "    numCols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    print(\"New numerical columns:\", numCols)\n",
    "    df[numCols] = df[numCols].fillna(df[numCols].median())\n",
    "\n",
    "    catCols = df.select_dtypes(exclude=np.number).columns\n",
    "    print(\"New categorical columns:\", catCols)\n",
    "    for col in catCols:\n",
    "        df[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    return numCols, catCols, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf71ac9",
   "metadata": {},
   "source": [
    "## Handling unique values and cardinality\n",
    "\n",
    "\n",
    "\"Each categorical variable consists of unique values. A categorical feature is said to possess high cardinality when there are too many of these unique values. One-Hot Encoding becomes a big problem in such a case since we have a separate column for each unique value (indicating its presence or absence) in the categorical variable. This leads to two problems, one is obviously space consumption, but this is not as big a problem as the second problem, the curse of dimensionality\" [reference here](https://towardsdatascience.com/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cardinality(column, threshold):\n",
    "    #threshold\n",
    "    threshold_value = int(threshold * len(column))\n",
    "    # Initialize\n",
    "    categories_list = []\n",
    "    s = 0\n",
    "    counts = []\n",
    "    \n",
    "    # Count the frequencies of unique values in the column\n",
    "    for value in column:\n",
    "        # Check if the value is already in the counts list\n",
    "        index = next((i for i, x in enumerate(counts) if x[0] == value), None)\n",
    "        if index is not None:\n",
    "            counts[index] = (value, counts[index][1] + 1)\n",
    "        else:\n",
    "            counts.append((value, 1))\n",
    "    \n",
    "    # Sort the list of tuples based on count in descending order\n",
    "    counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Loop through the tuples (value, count)\n",
    "    for i, j in counts:\n",
    "        # Add the frequency to the global sum\n",
    "        s += j\n",
    "        # Append the category name to the list\n",
    "        categories_list.append(i)\n",
    "        # Check if the global sum has reached the threshold value, if so break the loop\n",
    "        if s >= threshold_value:\n",
    "            break\n",
    "    \n",
    "    # Append the category 'Other' to the list\n",
    "    categories_list.append('Other')\n",
    "    \n",
    "    # Replace all instances not in our new categories by 'Other'\n",
    "    new_column=column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "    \n",
    "    #print(categories_list)\n",
    "    \n",
    "    return new_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e92f5",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811dae0",
   "metadata": {},
   "source": [
    "Important:  \n",
    "- Do first apply the `reduce_cardinality(df['Sector'])` method!\n",
    "- `add_dummies()`only for the catCols: `Sector`and `class` class should already be binary encoded! So dummies are only created for `Sector`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ece60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummies(df, catCols):\n",
    "    df = pd.get_dummies(df, columns=catCols)\n",
    "    #df.head()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6d4bc",
   "metadata": {},
   "source": [
    "# Part 2: Putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def putting_all_together(): \n",
    "    \n",
    "    # load dataset\n",
    "    df = load_dataset()\n",
    "    \n",
    "    # class to categorical\n",
    "    df = class_to_categorical(df)\n",
    "    \n",
    "    # check_duplicates_row(df)\n",
    "    # check_duplicates_col(df)\n",
    "    duplicated_columns = ['eBITperRevenue', 'eBTperEBIT', 'nIperEBT', 'Return on Tangible Assets', \n",
    "                     'ROIC', 'Payables Turnover', 'Inventory Turnover', 'Debt to Assets', 'Debt to Equity', \n",
    "                     'cashFlowCoverageRatios']\n",
    "    \n",
    "    # Remove duplicated columns\n",
    "    df = remove_duplicates(df, duplicated_columns)\n",
    "\n",
    "\n",
    "    # check correlation\n",
    "    abs_corr_unstack = correlation(df)\n",
    "\n",
    "    # Remove correlation\n",
    "    df = remove_correlation(df, abs_corr_unstack)\n",
    "\n",
    "    #adding indicators\n",
    "    df = adding_indicators(df)\n",
    "    \n",
    "    #dropping indicators who are obviously wrong---> this has to be optimized!!! \n",
    "    df = dropping_indicators(df)\n",
    "\n",
    "    # Remove missing values\n",
    "    numCols, catCols, df = handle_missing_data(df, 0.3)\n",
    "\n",
    "    df_reduced = df\n",
    "    df_reduced['Sector'] = reduce_cardinality(df['Sector'], 0.75)\n",
    "\n",
    "    df_dummies = df_reduced[df_reduced.columns.difference(['Class', 'year', 'PRICE VAR [%]', 'Stock Name'])]\n",
    "    df_dummies = add_dummies(df_dummies, ['Sector'])\n",
    "\n",
    "    X = df_dummies[df.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "    y = df['Class']\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce76f8",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = putting_all_together()\n",
    "y = y.astype('int')\n",
    "\n",
    "## Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "### Removing Outliers\n",
    "X_train, X_test, y_train, y_test = remove_outliers(X_train, X_test, y_train, y_test)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cef1c2",
   "metadata": {},
   "source": [
    "# Part 3: Feature Selection\n",
    "\n",
    "\n",
    "We implemented the following different feature selections:\n",
    "- `ExtraTreeClassifier`\n",
    "- `XGBClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076ae8a",
   "metadata": {},
   "source": [
    "### ExtraTrees Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eaab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "def get_significant_features(X_train, X_test, y_train, n):\n",
    "    # Feature selection using Extra Trees Classifier on the resampled training data\n",
    "    model = ExtraTreesClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "    importances_normalized = np.std([tree.feature_importances_ for tree in\n",
    "                                        model.estimators_],\n",
    "                                        axis = 0)\n",
    "\n",
    "    # Select top features with highest importance scores\n",
    "    top_features = pd.Series(importances, index=X_train.columns).nlargest(n)\n",
    "\n",
    "    # Subset X_resampled and X_test with selected features\n",
    "    X_train_selected = X_train[top_features.index]\n",
    "    X_test_selected = X_test[top_features.index]\n",
    "\n",
    "    return X_train_selected, X_test_selected, importances_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf0933",
   "metadata": {},
   "source": [
    "### Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "## Feature Selection using Random Forest for outside of pipeline\n",
    "def random_forest_feature_selection(X_train, X_test, y_train, y_test, n):\n",
    "    \n",
    "    model = SelectFromModel(RandomForestClassifier(n_estimators = n))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    list_train_rf= X_train.columns[(model.get_support())]\n",
    "    list_test_rf= X_test.columns[(model.get_support())]\n",
    "\n",
    "    X_train_rf = X_train[list_train_rf]\n",
    "    X_test_rf = X_test[list_test_rf]\n",
    "    \n",
    "    return X_train_rf, X_test_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13639eb9",
   "metadata": {},
   "source": [
    "### XG-Boost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "## Feature Selection using XGBoost for outside of pipeline\n",
    "def xg_boost_feature_selection(X_train, X_test, y_train, y_test, n):\n",
    "    \n",
    "    params = { \"objective\": \"multi:softmax\", 'num_class': 3 , 'random_state': 42 }\n",
    "    model= xgb.XGBClassifier(**params)\n",
    "    select_xgbc = SelectFromModel(estimator = model, threshold = \"median\")\n",
    "    select_xgbc.fit(X_train, y_train)\n",
    "\n",
    "    list_train_xgbc= X_train.columns[(select_xgbc.get_support())]\n",
    "    list_test_xgbc= X_test.columns[(select_xgbc.get_support())]\n",
    "\n",
    "\n",
    "    X_train_xgbc = X_train[list_train_xgbc]\n",
    "    X_test_xgbc = X_test[list_test_xgbc]\n",
    "    \n",
    "    return X_train_xgbc, X_test_xgbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e8891",
   "metadata": {},
   "source": [
    "# Part 4: Actual Machine Learning Models/Algos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ef26b",
   "metadata": {},
   "source": [
    "this function below need to be adjusted since its only copy pasted!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bf6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_results_crossvalidation(func, X_test, y_test):\n",
    "  \n",
    "  std_best_score = func.cv_results_[\"std_test_score\"][func.best_index_]\n",
    "  print(f\"Best parameters: {func.best_params_}\")\n",
    "  print(f\"Mean CV score: {func.best_score_:}\")\n",
    "  print(f\"Standard deviation of CV score: {std_best_score:}\")\n",
    "  print(\"Test Score:\".format(func.score(X_test, y_test)))\n",
    "\n",
    "def report(y_true, y_pred):\n",
    "    \n",
    "  class_report = classification_report(y_true, y_pred)\n",
    "  print(class_report)\n",
    "  conf_matrix = confusion_matrix(y_true, y_pred, normalize = \"true\")\n",
    "  conf_matrix = pd.DataFrame(conf_matrix, [\"Class 0\", \"Class 1\"],  [\"Class 0\", \"Class 1\"])\n",
    "  sns.heatmap(conf_matrix, annot = True).set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176befc",
   "metadata": {},
   "source": [
    "## Initalization of the pipeline and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90be082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "from sklearn.model_selection import  GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline and GridSearchCV\n",
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "kFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab6bb6",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07028e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b80776",
   "metadata": {},
   "source": [
    "First easy implementation all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e79030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize logistic regression classifier\n",
    "logistic = LogisticRegression(random_state=42, max_iter=20, n_jobs=-1) # initialize logistic regression classifier\n",
    "print(f\"Parameters of the logistic regression : {logistic.get_params().keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509aa8c7",
   "metadata": {},
   "source": [
    "Pipeline and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc47a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"classifier\", logistic]])\n",
    "# Set up parameter grid with hyperparameters we want to tune\n",
    "param_grid = {'ros': [ros, None], # upsampling or not\n",
    "              'scaler': [scaler, None, mms], # scaling input by standardizing or min-max scaling or not scaling at all\n",
    "              'classifier__C': [1, 6, 7, 8, 9, 10],\n",
    "              'classifier__penalty': [None, 'l2', 'l1', 'elasticnet']}\n",
    "\n",
    "# Conduct grid search with cross-validation to find hyperparameters that yield the best score\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "# Fit using the best parameters\n",
    "gs = gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f884c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing\n",
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183a647",
   "metadata": {},
   "source": [
    "###  Logistic Regression with Random Forest Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ec799",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c77733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator = forest, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"classifier\", logistic]])\n",
    "\n",
    "param_grid = {'ros': [ros, None],\n",
    "              'scaler': [scaler, mms, None],\n",
    "              'classifier__C': [1, 6, 7, 8, 9, 10],\n",
    "              'classifier__penalty': [None, 'l2', 'l1', 'elasticnet']}\n",
    "\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = 200)\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ad846",
   "metadata": {},
   "source": [
    "### Logistic Regression with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc= xgb.XGBClassifier(objective = \"multi:softmax\", random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator = xgbc, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"classifier\", logistic]])\n",
    "\n",
    "param_grid = {'ros': [ros, None],\n",
    "              'scaler': [scaler, mms, None],\n",
    "              'classifier__C': [1, 6, 7, 8, 9, 10],\n",
    "              'classifier__penalty': [None, 'l2', 'l1', 'elasticnet']}\n",
    "\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a22eb3",
   "metadata": {},
   "source": [
    "### Logistic Regression with Kernel PCA Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=42, max_iter=20, n_jobs=-1)\n",
    "kpca = KernelPCA(random_state = 42, eigen_solver = \"arpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"kpca\",  kpca],\n",
    "                          [\"ros\", ros], [\"classifier\", logistic]])\n",
    "\n",
    "param_grid = {'kpca__n_components': np.arange(5, 10, 1),\n",
    "              #linear is the \"normal\" PCA we have discussed in the lecture\n",
    "              #only consider linear and sigmoid since we have ran it before and saw that sigmoid normally performed better than poly and rbf\n",
    "              \"kpca__kernel\": [\"linear\", \"sigmoid\"],\n",
    "              'classifier__C': [1, 6, 7, 8, 9, 10]}\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold)\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ca107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "# get test score, metrics report and confusion matrix\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602853d",
   "metadata": {},
   "source": [
    "### Logistic Regression with Kernel PCA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=42, max_iter=20, n_jobs=-1, C = 9)\n",
    "kpca = KernelPCA(random_state = 42, eigen_solver = \"arpack\", n_components = 6, kernel = \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa72055",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"kpca\",  kpca], [\"ros\", ros], [\"classifier\", logistic]])                     \n",
    "\n",
    "param_grid = {'ros': [ros, None],\n",
    "              'scaler': [scaler, mms, None],\n",
    "              'classifier__penalty': [None, 'l2', 'l1', 'elasticnet']}\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold)\n",
    "gs = gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a928f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "# get test score, metrics report and confusion matrix\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77828673",
   "metadata": {},
   "source": [
    "### Feature Engineering with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce64d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=42, max_iter=20, n_jobs=-1) # initialize logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7573e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"classifier\", logistic]])\n",
    "# Set up parameter grid with hyperparameters we want to tune\n",
    "param_grid = {'ros': [ros, None], # upsampling or not\n",
    "              'scaler': [scaler, None, mms], # scaling input by standardizing or min-max scaling or not scaling at all\n",
    "              'classifier__C': [1, 6, 7, 8, 9, 10],\n",
    "              'classifier__penalty': [None, 'l2', 'l1', 'elasticnet']}\n",
    "# Conduct grid search with cross-validation to find hyperparameters that yield the best score\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "# Fit using the best parameters\n",
    "gs = gs.fit(X_train_fe, y_train_fe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test_fe, y_test_fe)\n",
    "y_pred = gs.best_estimator_.predict(X_test_fe)\n",
    "report(y_test_fe, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e43a3c",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae594c29",
   "metadata": {},
   "source": [
    "## Random Forest with all Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 42)\n",
    "print(f\"Parameters of the Random Forest: {forest.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b40778",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", forest]])\n",
    "\n",
    "random_grid = {\n",
    "    \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "    \"rf__min_samples_leaf\":np.array([1, 2, 5]),\n",
    "    \"rf__min_samples_split\": np.array([2, 5, 10]),\n",
    "    \"rf__n_estimators\": np.array([50, 100, 200, 500]),\n",
    "    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = rfpipe, param_distributions = random_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, n_iter = 100, random_state = 42, error_score = \"raise\")\n",
    "rs = rs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(rs, X_test, y_test)\n",
    "y_pred = rs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa44153a",
   "metadata": {},
   "source": [
    "### Random Forest with All Features but also Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 42, class_weight = \"balanced_subsample\", criterion = \"entropy\", max_features = \"sqrt\", min_samples_split = 10, min_samples_leaf = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fffabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"rf\", forest]])\n",
    "\n",
    "param_grid = {\n",
    "    \"scaler\": [scaler, mms, None],\n",
    "    \"ros\": [ros, None],\n",
    "    \"rf__max_depth\": [None, 5, 10, 15, 20],\n",
    "    \"rf__n_estimators\": np.array([150, 200, 250])\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator = rfpipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1081de",
   "metadata": {},
   "source": [
    "## Random Forest Feature Selectionn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the pipeline implementation, we use the prepared data sets X_train_rf and X_test_rf\n",
    "forest = RandomForestClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0220ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", forest]])\n",
    "\n",
    "random_grid = {\n",
    "    \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "    \"rf__min_samples_leaf\":np.array([1, 2, 5]),\n",
    "    \"rf__min_samples_split\": np.array([2, 5, 10]),\n",
    "    \"rf__n_estimators\": np.array([ 100, 200, 500]),\n",
    "    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = rfpipe, param_distributions = random_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, n_iter = 100, random_state = 42, error_score = \"raise\")\n",
    "rs = rs.fit(X_train_rf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(rs, X_test_rf, y_test)\n",
    "y_pred = rs.best_estimator_.predict(X_test_rf)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf59f69",
   "metadata": {},
   "source": [
    "### Random Forest with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the pipeline implementation, we use the prepared data sets X_train_xgbc and X_test_xgbc\n",
    "forest = RandomForestClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7efe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", forest]])\n",
    "\n",
    "random_grid = {\n",
    "    \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "    \"rf__min_samples_leaf\":np.array([1, 2, 5]),\n",
    "    \"rf__min_samples_split\": np.array([2, 5, 10]),\n",
    "    \"rf__n_estimators\": np.array([ 100, 200]),\n",
    "    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = rfpipe, param_distributions = random_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, n_iter = 100 random_state = 42, error_score = \"raise\")\n",
    "rs = rs.fit(X_train_xgbc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75716b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(rs, X_test_xgbc, y_test)\n",
    "y_pred = rs.best_estimator_.predict(X_test_xgbc)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f608331",
   "metadata": {},
   "source": [
    "### Random Forest with Kernel PCA: Broad Hyperparametertuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 42, max_features = \"sqrt\")\n",
    "kpca = KernelPCA(random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73025b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"scaler\", scaler],[\"kpca\", kpca], [\"ros\", ros], [\"rf\", forest]])\n",
    "\n",
    "random_grid = {\n",
    "    \"kpca__n_components\": np.arange(4, 10, 1),\n",
    "    \"kpca__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"kpca__gamma\": np.linspace(0.005, 0.01, 5),\n",
    "    \"kpca__coef0\": np.linspace(0.8, 1.2, 5),\n",
    "    \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"rf__max_depth\": np.array([None, 2, 5, 10, 20]),\n",
    "    \"rf__n_estimators\": np.array([ 100, 200, 500]),\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = rfpipe, param_distributions = random_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, n_iter = 100, random_state = 42, error_score = \"raise\")\n",
    "rs = rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(rs, X_test, y_test)\n",
    "y_pred = rs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605ac2d",
   "metadata": {},
   "source": [
    "## Random Forest with Kernel PCA: Hyperparameter Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cbd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_ft = RandomForestClassifier(random_state = 42, class_weight = \"balanced_subsample\", criterion = \"gini\", \n",
    "                                   max_features = \"sqrt\", min_samples_leaf= 10, min_samples_split = 5)\n",
    "kpca_rf_ft = KernelPCA(random_state = 42, kernel = \"sigmoid\", n_components = 5, gamma = 0.01, coef0 = 0.9)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"scaler\", scaler],[\"kpca\", kpca_rf_ft], [\"ros\", ros], [\"rf\", forest_ft]])\n",
    "\n",
    "param_grid = {\n",
    "    \"scaler\": [scaler, mms, None],\n",
    "    \"ros\": [ros, None],\n",
    "    \"rf__max_depth\": np.array([15, 20, 25]),\n",
    "    \"rf__n_estimators\": np.array([100, 300, 500]),\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator = rfpipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb512aa",
   "metadata": {},
   "source": [
    "### Random Forest with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2dc98",
   "metadata": {},
   "source": [
    "all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ad1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7845296",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", forest]]) # we saw that random forest performed better when not scaling the input\n",
    "\n",
    "random_grid = {\n",
    "    \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "    \"rf__min_samples_leaf\":np.array([1, 2, 5]),\n",
    "    \"rf__min_samples_split\": np.array([2, 5, 10]),\n",
    "    \"rf__n_estimators\": np.array([50, 100, 200]),\n",
    "    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(estimator = rfpipe, param_distributions = random_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, n_iter = 100, random_state = 42, error_score = \"raise\")\n",
    "rs = rs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a882d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(rs, X_test, y_test)\n",
    "y_pred = rs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a504bfa",
   "metadata": {},
   "source": [
    "### Random Forest Feature Engineering Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ca3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 42, class_weight = \"balanced_subsample\", criterion = \"gini\", max_features = \"sqrt\", min_samples_split = 10, min_samples_leaf = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfpipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", forest]])\n",
    "\n",
    "param_grid = {\n",
    "    \"rf__max_depth\": [None, 20],\n",
    "    \"rf__n_estimators\": np.array([150, 200, 250])\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator = rfpipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253531f8",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "#from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = putting_all_together()\n",
    "y = y.astype('int')\n",
    "\n",
    "## Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "### Removing Outliers\n",
    "X_train, X_test, y_train, y_test = remove_outliers(X_train, X_test, y_train, y_test)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d764f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SVC Class from Sklearn\n",
    "svm1= svm.SVC(\n",
    "        C=1.0,                          # The regularization parameter\n",
    "        kernel='rbf',                   # The kernel type used \n",
    "        degree=3,                       # Degree of polynomial function \n",
    "        gamma='scale',                  # The kernel coefficient\n",
    "        coef0=0.0,                      # If kernel = 'poly'/'sigmoid'\n",
    "        shrinking=True,                 # To use shrinking heuristic\n",
    "        probability=False,              # Enable probability estimates\n",
    "        tol=0.001,                      # Stopping crierion\n",
    "        cache_size=200,                 # Size of kernel cache\n",
    "        class_weight=None,              # The weight of each class\n",
    "        verbose=False,                  # Enable verbose output\n",
    "        max_iter= -1,                   # Hard limit on iterations\n",
    "        decision_function_shape='ovr',  # One-vs-rest or one-vs-one\n",
    "        break_ties=False,               # How to handle breaking ties\n",
    "        random_state=42               # Random state of the model\n",
    "    )\n",
    "\n",
    "print(f\"Parameters of the Support Vector Machine: {svm1.get_params().keys()}\")\n",
    "\n",
    "\n",
    "# Building and training our model\n",
    "clf = svm1.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with our data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accurancy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\", precision_score(y_true= y_test, y_pred= predictions, average= 'weighted')) # WEIGHTED???? \n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\", recall_score(y_test, predictions, average= 'weighted'))  # WEIGHTED???? \n",
    "\n",
    "#Whole classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "kFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de34bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"SVM\", svm1]])\n",
    "param_grid = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\", \"rbf\"],\n",
    "    \"SVM__C\": [1, 5, 10, 50],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator = svm_pipe, param_grid = param_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, refit = True, verbose = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a822d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time gs.fit(X_train, y_train)\n",
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a88cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting a Bar Graph to compare the models\n",
    "# plt.bar(X_train.columns, importances)\n",
    "# plt.xlabel('Feature Labels')\n",
    "# plt.ylabel('Feature Importances')\n",
    "# plt.title('Comparison of different Feature Importances')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StandardScaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Standardize features; equal results as if done in two\n",
    "# X_train_std = sc.fit_transform(X_train_selected)\n",
    "# X_test_std = sc.transform(X_test_selected)\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid Search for getting optimal C and gamma\n",
    "# gamma_range = np.outer(np.logspace(-3, 0, 4),np.array([1,5]))\n",
    "# gamma_range = gamma_range.flatten()\n",
    "# print(gamma_range)\n",
    "\n",
    "# C_range = np.outer(np.logspace(-1, 1, 3),np.array([1,5]))\n",
    "# C_range = C_range.flatten()\n",
    "# print(C_range)\n",
    "\n",
    "# parameters = {'kernel':['linear', 'rbf'], 'C':C_range, 'gamma': gamma_range}\n",
    "\n",
    "# svm = SVC()\n",
    "# grid = RandomizedSearchCV(estimator=svm, param_distributions=parameters, n_iter=5, n_jobs=-1, verbose=2)\n",
    "# grid.fit(X_train_std, y_train)\n",
    "\n",
    "# print('Best CV accuracy: {:.2f}'.format(grid.best_score_))\n",
    "# print('Test score:       {:.2f}'.format(grid.score(X_test_std, y_test)))\n",
    "# print('Best parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0)\n",
    "svm.fit(X_train_std , y_train)\n",
    "\n",
    "# Predict classes and print results\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Test score: {:.2f}\".format(svm.score(X_test_std, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
