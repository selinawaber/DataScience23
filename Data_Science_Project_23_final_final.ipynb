{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas_datareader import data\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80267f0",
   "metadata": {},
   "source": [
    "## Our Goal\n",
    "\n",
    "The efficient market hypothesis states that - if asset prices reflect all available information - it should not be possible to  be superior to the market in the long run (\"beat the market\"). However, does this hypothesis really holds? Based on the 10-K filings, we try to predict the market by applying different maschine learning algorithms. We apply Naive Bayes, Random Forest, Logistic Regression and SVM Classifer and try to answer the following questions: \n",
    "- Which algorithms yield the best prediction scores? \n",
    "- Can one improves the prediction by applying feature selection algorithms?\n",
    "- Which features are most relevant to predict the stock market?\n",
    "\n",
    "Finally, we build a feedforward neural network. However, we could not include it into our general pipeline and leave it therefore as an addendum.\n",
    "\n",
    "\n",
    "### Data set\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018.\n",
    "\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "Overall we agreed to use and define classes and functions as much as possible. Using a class can make your code cleaner, easier to manage and debug, and more efficient, particularly when dealing with complex data processing tasks.\n",
    "\n",
    "We have defined the following classes:\n",
    "- Dataset\n",
    "- TrainModel\n",
    "\n",
    "They will be explained in more detail below. \n",
    "\n",
    "Further we defined the following functions outside our classes specifically for our feature selection:\n",
    "- get_significant_features\n",
    "- random_forest_feature_selection\n",
    "- xg_boost_feature_selection\n",
    "- pca_feature_selection \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ef7cb",
   "metadata": {},
   "source": [
    "## Part 1: The class Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb40fb0",
   "metadata": {},
   "source": [
    "We define a class `Dataset` to handle all stages of pre-processing and exploratory data analysis (EDA) for our financial dataset. The steps performed by this class can be grouped into several categories:\n",
    "\n",
    "1) Data loading and merging: The `load_dataset` function loads multiple yearly datasets from csv files located in the specified path, and concatenates them into one large pandas DataFrame. The `load_small_dataset` function does a similar operation but for a single file.\n",
    "\n",
    "2) Data Cleaning: This includes handling duplicate data (`remove_duplicates`), handling missing data (`handle_missing_data`), and removing outliers (`remove_outliers`). The outliers removal process uses the Isolation Forest algorithm.\n",
    "\n",
    "3) Feature Engineering: The class has several methods (`adding_indicators`, `dropping_indicators`, `remove_correlation`) for generating new features (such as `cashflow_margin` and `RONA`), and removing highly correlated features.\n",
    "\n",
    "4) Data transformation: The class provides functionality to convert categorical variables to dummy variables (`add_dummies`), and reduce the cardinality of categorical variables (`reduce_cardinality`).\n",
    "\n",
    "5) Dataset splitting: The `split_dataset` function splits the dataset into training and testing sets using stratified sampling.\n",
    "\n",
    "6) EDA: The `do_eda` function integrates various functions to perform a complete exploratory data analysis, including removing duplicates, removing correlated features, feature engineering, dropping certain indicators, handling missing data, and converting categorical variables into dummy variables. It also provides an option to reduce the cardinality of certain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, path, years):\n",
    "        if type(years) == list:    \n",
    "            self.data = self.load_dataset(path, years)\n",
    "        elif type(years) == int:\n",
    "            self.data = self.load_small_dataset(path, years)\n",
    "        else: raise TypeError(\"Wrong input for the data loading\")            \n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.train_X = None\n",
    "        self.test_X = None\n",
    "        self.train_y = None\n",
    "        self.test_y = None\n",
    "\n",
    "    def load_dataset(self, path, years):\n",
    "        ## Loading the yearly dataset into the array dfs\n",
    "        dfs = []\n",
    "        for year in years:\n",
    "            df = pd.read_csv(os.path.join(path, f'{year}_Financial_Data.csv'), sep=',')\n",
    "            df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "            df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "            df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "            df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "            dfs.append(df)\n",
    "        \n",
    "        df = pd.concat(dfs, ignore_index=True) ## concat the diffrent dataframes\n",
    "        return df\n",
    "\n",
    "    def load_small_dataset(self, path, year):\n",
    "        project_directory = sys.path[0] ## get path of project directory\n",
    "        data_directory = os.path.join(project_directory, 'data')\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "        df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "        df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "        df = df.drop(columns=[f'{year +1} PRICE VAR [%]'])\n",
    "        df.columns.values[0] = 'Stock Name' # name the first variable\n",
    "        return df\n",
    "    \n",
    "    def class_to_categorical(self):\n",
    "        self.data['Class'] = self.data.Class.astype('object') ## convert 'Class' to categorical data as an object.\n",
    "\n",
    "    def remove_duplicates(self, columns):\n",
    "        self.data=self.data.drop(columns=columns)\n",
    "    \n",
    "    def adding_indicators(self):\n",
    "        df = self.data\n",
    "        ## Yearly Means of S&P 500\n",
    "        sp500_means = pd.Series([11.39, -0.73, 9.54, 19.42, -6.24], index = [2014, 2015, 2016, 2017, 2018]) \n",
    "        \n",
    "        # Yearly Inflaction Rate measured by consumer price index\n",
    "        inflation = pd.Series([1.62, 0.12, 1.26, 2.13, 2.44], index = [2014, 2015, 2016, 2017, 2018]) \n",
    "        \n",
    "        ##Adding to the dataframe\n",
    "        df[\"inflation\"] = df.apply(lambda x: inflation[int(x[\"year\"])], axis=1)\n",
    "        df[\"sp500_means\"] = df.apply(lambda x: sp500_means[int(x[\"year\"])], axis=1)\n",
    "       \n",
    "        ## Calculation of Cashflow Margin:\n",
    "        df[\"cashflow_margin\"] = df[\"Operating Cash Flow\"].divide(df[\"Revenue\"])\n",
    "        # Pay attention to ZeroDivisionError, replace infinity by NAN\n",
    "        df[\"cashflow_margin\"] = df[\"cashflow_margin\"].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        ## Calculation of Return on Net Assets (RONA)\n",
    "        df[\"Net_working_capital\"] = df[\"Total assets\"]-df[\"Cash and cash equivalents\"]\n",
    "        df[\"RONA\"] = df[\"EBIT\"]/df[\"Net_working_capital\"]\n",
    "        df[\"RONA\"] = df[\"RONA\"].replace([np.inf, -np.inf], np.nan)\n",
    "        self.data = df\n",
    "    \n",
    "    def dropping_indicators(self):\n",
    "        self.data = self.data.drop([\"operatingProfitMargin\"], axis = 1) # consisting only of the value 1.\n",
    "\n",
    "    def remove_correlation(self):\n",
    "        X = self.data[self.data.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "        y = self.data[\"Class\"]\n",
    "\n",
    "        abs_corr = X.corr().abs()\n",
    "        for i in range(len(abs_corr)):\n",
    "            abs_corr.iloc[i, i] = 0\n",
    "\n",
    "        abs_corr_unstack = abs_corr.unstack()\n",
    "        columns_to_drop = []\n",
    "        columns_to_remain = []\n",
    "\n",
    "        for pair in abs_corr_unstack.index.values:\n",
    "            if abs_corr_unstack[pair] > 0.99:\n",
    "                if pair[0] not in columns_to_remain and pair[1] not in columns_to_remain:\n",
    "                        columns_to_remain.append(pair[0])\n",
    "                        if pair[1] not in columns_to_drop:\n",
    "                            columns_to_drop.append(pair[1])\n",
    "                elif pair[0] in columns_to_remain:\n",
    "                    if pair[1] not in columns_to_drop:\n",
    "                        columns_to_drop.append(pair[1])\n",
    "                elif pair[1] in columns_to_remain:\n",
    "                    if pair[0] not in columns_to_drop:\n",
    "                        columns_to_drop.append(pair[0])\n",
    "\n",
    "        self.data = self.data.drop(columns=columns_to_drop)\n",
    "    \n",
    "    def remove_outliers(self):\n",
    "        # Isolation Forest\n",
    "        outliers = IsolationForest(random_state = 42).fit(self.X_train) # fit Isolation Forest only to training data\n",
    "        outliers_train = outliers.predict(self.X_train)\n",
    "        outliers_test = outliers.predict(self.X_test)\n",
    "\n",
    "        # Remove outliers where 1 represent inliers and -1 represent outliers:\n",
    "        self.X_train = self.X_train[np.where(outliers_train == 1, True, False)]\n",
    "        self.y_train = self.y_train[np.where(outliers_train == 1, True, False)]\n",
    "        self.X_test = self.X_test[np.where(outliers_test == 1, True, False)]\n",
    "        self.y_test = self.y_test[np.where(outliers_test == 1, True, False)]\n",
    "\n",
    "    def handle_missing_data(self, threshold=0.3, imputer=\"median\"):\n",
    "        df = self.data\n",
    "        # Calculate the percentage of missing values in each column\n",
    "        missing_values_percentage = df.isna().sum() / df.shape[0]\n",
    "        # Identify the columns with a higher percentage of missing values than the threshold\n",
    "        columns_to_drop = missing_values_percentage[missing_values_percentage > threshold].index\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "        if imputer == \"median\":\n",
    "            numCols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "            df[numCols] = df[numCols].fillna(df[numCols].median())\n",
    "            catCols = df.select_dtypes(exclude=np.number).columns\n",
    "        for col in catCols:\n",
    "            df[col].fillna(\"Unknown\", inplace=True)  \n",
    "            missing_values_percentage = df.isna().sum() / df.shape[0]\n",
    "            columns_to_drop = missing_values_percentage[missing_values_percentage > threshold].index\n",
    "\n",
    "        if imputer == \"KNN\":\n",
    "            X = df.drop(columns = [\"PRICE VAR [%]\", \"Class\", \"year\", \"Sector\", \"Stock Name\"])\n",
    "\n",
    "            imputer = KNNImputer(n_neighbors = 5, weights = \"distance\")\n",
    "            X = imputer.fit_transform(X)\n",
    "            X = pd.DataFrame(X, columns=df.columns.drop([\"PRICE VAR [%]\", \"Class\", \"year\", \"Sector\", \"Stock Name\"]))\n",
    "            df[X.columns] = X\n",
    "        \n",
    "        self.data = df\n",
    "    \n",
    "    def reduce_cardinality(self, column, threshold):\n",
    "        threshold_value = int(threshold * len(column))\n",
    "        categories_list = []\n",
    "        s = 0\n",
    "        counts = []\n",
    "        \n",
    "        # Count the frequencies of unique values in the column\n",
    "        for value in column:\n",
    "            # Check if the value is already in the counts list\n",
    "            index = next((i for i, x in enumerate(counts) if x[0] == value), None)\n",
    "            if index is not None:\n",
    "                counts[index] = (value, counts[index][1] + 1)\n",
    "            else:\n",
    "                counts.append((value, 1))\n",
    "        \n",
    "        # Sort the list of tuples based on count in descending order\n",
    "        counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Loop through the tuples (value, count)\n",
    "        for i, j in counts:\n",
    "            # Add the frequency to the global sum\n",
    "            s += j\n",
    "            # Append the category name to the list\n",
    "            categories_list.append(i)\n",
    "            # Check if the global sum has reached the threshold value, if so break the loop\n",
    "            if s >= threshold_value:\n",
    "                break\n",
    "        \n",
    "        # Append the category 'Other' to the list\n",
    "        categories_list.append('Other')\n",
    "        print(categories_list)\n",
    "        new_column = column.apply(lambda x: x if x in categories_list else 'Other')\n",
    "        self.data['Sector'] = new_column\n",
    "\n",
    "    def add_dummies(self, catCols):\n",
    "        df_dummies = pd.get_dummies(self.data, columns=catCols)\n",
    "        self.data = df_dummies\n",
    "    \n",
    "    def do_eda(self, remove_duplicates=True, feature_eng=True, remove_corr=True, missing_thresh=0.3, reduce_col=['Sector'], reduce_thresh=0.75, imputer = \"median\"):\n",
    "        self.class_to_categorical()\n",
    "        \n",
    "        if remove_duplicates:\n",
    "            duplicated_columns = ['eBITperRevenue', 'eBTperEBIT', 'nIperEBT', 'Return on Tangible Assets', \n",
    "                     'ROIC', 'Payables Turnover', 'Inventory Turnover', 'Debt to Assets', 'Debt to Equity', \n",
    "                     'cashFlowCoverageRatios']\n",
    "            self.remove_duplicates(duplicated_columns)\n",
    "        \n",
    "        if remove_corr:\n",
    "            self.remove_correlation()\n",
    "        \n",
    "        if feature_eng:\n",
    "            self.adding_indicators()\n",
    "        \n",
    "        self.dropping_indicators()\n",
    "        self.handle_missing_data(missing_thresh, imputer)\n",
    "        #self.reduce_cardinality(self.data['Sector'], reduce_thresh)\n",
    "        self.add_dummies(reduce_col)\n",
    "        \n",
    "    def split_dataset(self, remove_outliers=True):\n",
    "        self.y = self.data['Class'].astype('int')\n",
    "        self.X = self.data[self.data.columns.difference(['Class', 'Stock Name', 'Sector', 'year', 'PRICE VAR [%]'])]\n",
    "\n",
    "        ## Train-Test Split\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, stratify = self.y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "        if remove_outliers:\n",
    "            self.remove_outliers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174a163",
   "metadata": {},
   "source": [
    "## Explanation of the variables: \n",
    "\n",
    "\n",
    "### Adding `year` as a categorical variable\n",
    "\n",
    "After loading the data set, we add a column named `year` which is used for the exploratory data analysis. However, for the training of our models the variable year is dropped again.\n",
    "\n",
    "### Handling the variable `Price VAR [%]`\n",
    "\n",
    "The data set contains a column `PRICE VAR [%]`, which lists the percent price variation of each stock for the following year after the 10-K filings were published. Since this variable is listed in the original data set with the specific year, we had to rename it, e.g. `2016 PRICE VAR [%]` to `PRICE VAR [%]`.\n",
    "\n",
    "### The variable `class`\n",
    "\n",
    "Our maschine learning algorithms are trained to classify if the stock price is falling or rising. This information is listed under the categorial variabel class `PRICE VAR [%]`. If the class variable is 1, the stock gained value in the year after the corresponding 10-K filings were published. A hypothetical trader should have bought this stock. Conversely, a value of 0 indicates that the value is decreasing and that a hypothetical trader should not have bought this stock.\n",
    "\n",
    "The two features `PRICE VAR [%]` and `class` would allow both classification and regression tasks. However, since we cannot quantify the uncertainty of a `PRICE VAR [%]` prediction, the hypothetical trader does not gain any information by regression compared to classification. Therefore, we restrict ourselves on classification algorithms.\n",
    "\n",
    "\n",
    "### Duplicates\n",
    "\n",
    "During the process of data cleaning, we detected several doublicated columns but no doublicated rows. Our Duplicates are the following pairs:\n",
    "\n",
    "- `ebitperRevenue` and `eBITperRevenu`\n",
    "- `ebtperEBIT` and `eBTperEBIT`\n",
    "- `niperEBT` and `nIperEBT`\n",
    "- `returnOnAssets` and `Return on Tangible Assets`\n",
    "- `returnOnCapitalEmployed` and `ROIC`\n",
    "- `payablesTurnover` and `Payables Turnover`\n",
    "- `inventoryTurnover` and `Inventory Turnover`\n",
    "- `debtRatio` and `Debt to Assets`\n",
    "- `debtEquityRatio` and `Debt to Equity`\n",
    "- `cashFlowToDebtRatio` and `cashFlowCoverageRatios`\n",
    "\n",
    "### Outliers cleaning and Missing values\n",
    "\n",
    "Furthermore, we notice many missing values in our data set. We decided to drop columns with more than 30% of missing values and use a KNN imputer for the remaining ones. Additional to the missing data, many outliers were found which may reduce the quality of the data. However, considering financial indicators, it is not impossible that strong outliers reflect true values. Therefore, the handling of outliers is not obvious at all. We decided to use a Isolation Forest algorithm to remove the outliers. \n",
    "\n",
    "### Handling unique values and cardinality\n",
    "\n",
    "Our data set contains one categorial variable called `Sector` which indicates in which field the company operates. Some of the values of the `Sector` variable are very weakly populated. To prevent the creation of unnecessary dummy variables, we had to reduce the cardinality. A short description of cardinality is given in the following: \"Each categorical variable consists of unique values. A categorical feature is said to possess high cardinality when there are too many of these unique values. One-Hot Encoding becomes a big problem in such a case since we have a separate column for each unique value (indicating its presence or absence) in the categorical variable. This leads to two problems, one is obviously space consumption, but this is not as big a problem as the second problem, the curse of dimensionality\" [reference here](https://towardsdatascience.com/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cef1c2",
   "metadata": {},
   "source": [
    "# Part 2: Feature Selection Functions\n",
    "\n",
    "\n",
    "We implemented the following different feature selections functions:\n",
    "1) Extra Tree\n",
    "2) Random Forest\n",
    "3) Xg Boost\n",
    "4) PCA\n",
    "\n",
    "\n",
    "### Why Feature Selection?\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features (variables or predictors). It is a critical step that can have a profound impact on the performance of your model. The main benefits of feature selection include:\n",
    "- Simpler Models: \n",
    "- Less Overfitting: \n",
    "- Speeds Up Training: \n",
    "- Improved Accuracy: \n",
    "- Reduces Noise:\n",
    "- Prevents Multicollinearity: \n",
    "\n",
    "### Difference Random Forest and Xg-Boost:\n",
    "\n",
    "- Random Forest measures feature importance based on the average decrease in impurity (Gini or Entropy based on the chosen criterion) that results from splits over each feature. This is averaged over all trees in the forest. The random forest algorithm uses bagging (bootstrap aggregating), where different subsets of the data are used to create each decision tree. As a result, it tends to give a more robust estimate of feature importance which is less likely to be influenced by noise or outliers.\n",
    "\n",
    "- XGBoost, on the other hand, calculates feature importance based on the number of times a feature appears in a tree across all trees in the model (weight), the average gain of splits which use the feature (gain), or the total number of instances split on this feature (coverage). XGBoost uses a concept called boosting, where each new tree is created to correct the errors made by the existing set of trees. As a result, it can model complex relationships and may assign higher importance to features that appear in complex interactions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a60ef",
   "metadata": {},
   "source": [
    "It's also important to remember that feature importance doesn't necessarily imply causality; a feature may be important in the context of a particular model without being a causal factor for the outcome being predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076ae8a",
   "metadata": {},
   "source": [
    "### 1) ExtraTrees Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eaab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_features(X_train, X_test, y_train, n):\n",
    "    # Feature selection using Extra Trees Classifier on the resampled training data\n",
    "    model = ExtraTreesClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "    importances_normalized = np.std([tree.feature_importances_ for tree in\n",
    "                                        model.estimators_],\n",
    "                                        axis = 0)\n",
    "\n",
    "    # Select top features with highest importance scores\n",
    "    top_features = pd.Series(importances, index=X_train.columns).nlargest(n)\n",
    "\n",
    "    # Subset X_resampled and X_test with selected features\n",
    "    X_train_selected = X_train[top_features.index]\n",
    "    X_test_selected = X_test[top_features.index]\n",
    "\n",
    "    return X_train_selected, X_test_selected, importances_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf0933",
   "metadata": {},
   "source": [
    "### 2) Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_feature_selection(X_train, X_test, y_train, y_test, n):\n",
    "    \n",
    "    model = SelectFromModel(RandomForestClassifier(n_estimators = n))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    list_train_rf= X_train.columns[(model.get_support())]\n",
    "    list_test_rf= X_test.columns[(model.get_support())]\n",
    "\n",
    "    X_train_rf = X_train[list_train_rf]\n",
    "    X_test_rf = X_test[list_test_rf]\n",
    "    \n",
    "    return X_train_rf, X_test_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13639eb9",
   "metadata": {},
   "source": [
    "### 3) XG-Boost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg_boost_feature_selection(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    params = { \"objective\": \"multi:softmax\", 'num_class': 2 , 'random_state': 42 }\n",
    "    model= xgb.XGBClassifier(**params)\n",
    "    select_xgbc = SelectFromModel(estimator = model, threshold = \"median\")\n",
    "    select_xgbc.fit(X_train, y_train)\n",
    "\n",
    "    list_train_xgbc= X_train.columns[(select_xgbc.get_support())]\n",
    "    list_test_xgbc= X_test.columns[(select_xgbc.get_support())]\n",
    "\n",
    "\n",
    "    X_train_xgbc = X_train[list_train_xgbc]\n",
    "    X_test_xgbc = X_test[list_test_xgbc]\n",
    "    \n",
    "    return X_train_xgbc, X_test_xgbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3428303",
   "metadata": {},
   "source": [
    "### 4) PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9365386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_feature_selection(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create a PCA object and fit it to the scaled data\n",
    "    pca = PCA(n_components=3) # Select the number of components you want to keep\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Transform the data to the selected number of components\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    # Train the logistic regression model on the transformed data\n",
    "    logModel_pca = LogisticRegression().fit(X_train_pca, y_train)\n",
    "    y_pred_pca = logModel_pca.predict(X_test_pca)\n",
    "\n",
    "    return X_train_pca, X_test_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e8891",
   "metadata": {},
   "source": [
    "# Part 3: Actual Machine Learning Models/Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f84043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing dataset path\n",
    "path = os.path.join(sys.path[0], 'data')\n",
    "years = [2014, 2015, 2016, 2017, 2018] # Years to include in the training\n",
    "\n",
    "# Loading and Handling dataset\n",
    "df = Dataset(path, years)\n",
    "df.do_eda(feature_eng=True, missing_thresh=0.3, reduce_thresh=0.75, imputer = \"median\")\n",
    "\n",
    "# Splitting dataset into training and testing dataset\n",
    "df.split_dataset()\n",
    "X_train, X_test, y_train, y_test = df.X_train, df.X_test, df.y_train, df.y_test\n",
    "\n",
    "# Unified parameters for pipeline's random search\n",
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "\n",
    "# Standardize features; equal results as if done in two\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa44a9",
   "metadata": {},
   "source": [
    "## The class TrainModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d31c0",
   "metadata": {},
   "source": [
    "Here is our class `TrainModel`. This class is used for training our different models. It consists of the following parts:\n",
    "\n",
    "1) The __init__ method: This is the constructor that initializes the TrainModel instance. It takes four arguments:\n",
    "\n",
    "- model, which is the model to be trained,\n",
    "- model_params, which are the hyperparameters for the model,\n",
    "- pipeline, which is a series of data preprocessing steps, and\n",
    "- pipeline_params, which are the parameters for the pipeline steps.\n",
    "- train_model method: This method trains the model using RandomizedSearchCV which performs hyperparameter tuning. If -- use_std is set to True, it uses standardized training data (X_train_std) for the model training, else it uses normal training data (X_train).\n",
    "\n",
    "2) train_pipeline method: This method trains the entire pipeline using Randomized Search Cross Validation. The best parameters found are printed.\n",
    "\n",
    "3) get_results method: After the model has been trained, this method is used to get the model's predictions on the test set. If use_std is True, it uses standardized test data (X_test_std), otherwise it uses the normal test data (X_test). The predictions are then passed to the report method.\n",
    "\n",
    "4) report method: This method prints a classification report which includes precision, recall, f1-score, and support for each class. It also prints and plots a confusion matrix using seaborn's heatmap, which shows the number of correct and incorrect predictions made by the model.\n",
    "\n",
    "In this code, it's important to note that X_train, X_test, y_train, y_test, X_train_std, X_test_std should be defined in the global scope for this class to work, as they are not passed as arguments to the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "    def __init__(self, model, model_params, pipeline, pipeline_params):\n",
    "        self.model = model\n",
    "        self.pipe = pipeline\n",
    "\n",
    "        # Set uniform params for pipeline Randomized Search\n",
    "        self.pipeline_params = pipeline_params\n",
    "\n",
    "        # Set params for model Randomized Search\n",
    "        self.model_params = model_params\n",
    "        \n",
    "        self.kFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "    \n",
    "    def train_model(self, use_std=False):\n",
    "        random = RandomizedSearchCV(estimator=self.model, param_distributions=self.model_params, n_iter=1, verbose=10, random_state=42, n_jobs=-1)\n",
    "        if use_std:\n",
    "            print(\"used_std\")\n",
    "            self.model = random.fit(X_train_std, y_train)\n",
    "        else:\n",
    "            self.model = random.fit(X_train, y_train)\n",
    "        print(\"Best model params used:\", self.model.best_params_)\n",
    "\n",
    "    def train_pipeline(self, use_std=False):\n",
    "        random = RandomizedSearchCV(estimator=self.pipe, param_distributions=self.pipeline_params, scoring = \"f1_weighted\", cv = self.kFold, n_jobs = -1, verbose=10)\n",
    "        if use_std:\n",
    "            print(\"used_std\")\n",
    "            self.model = random.fit(X_train_std, y_train)\n",
    "        else:\n",
    "            self.model = random.fit(X_train, y_train)\n",
    "        print(\"Best pipeline params used:\", self.model.best_params_)\n",
    "\n",
    "    def get_results(self, use_std=False, plot=True):\n",
    "        if use_std:\n",
    "            print(\"used_std\")\n",
    "            y_pred = self.model.best_estimator_.predict(X_test_std)\n",
    "        else:\n",
    "            y_pred = self.model.best_estimator_.predict(X_test)\n",
    "        self.report(y_test, y_pred, plot)\n",
    "\n",
    "    def report(self, y_test, y_pred, plot=True):\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(class_report)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred, normalize = \"true\")\n",
    "        print(conf_matrix)\n",
    "        conf_matrix = pd.DataFrame(conf_matrix, [\"Class 0\", \"Class 1\"],  [\"Class 0\", \"Class 1\"])\n",
    "        if plot:    \n",
    "            sns.heatmap(conf_matrix, annot = True).set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749c7f1",
   "metadata": {},
   "source": [
    "### Scaling our Data\n",
    "\n",
    "Scaling is a necessary pre-processing step for certain machine learning algorithms, especially those that rely on the calculation of distances or optimization methods.\n",
    "\n",
    "Here are some examples of algorithms where scaling is crucial:\n",
    "- Linear and Logistic Regression: When regularization is used (like Ridge or Lasso), features need to be on the same scale since regularization penalizes larger weights more heavily.\n",
    "\n",
    "- Support Vector Machines (SVM): SVMs try to maximize the margin between different classes. Feature scaling can have a significant impact on the margin size and therefore, the SVM's performance.\n",
    "\n",
    "- K-Nearest Neighbors (K-NN): This algorithm calculates the distance between different points. Scaling ensures that all features contribute equally to the distance calculation.\n",
    "\n",
    "- Neural Networks: The weights of neural networks are updated using optimization methods like gradient descent. Feature scaling can make the surface of the loss function smoother, leading to quicker convergence.\n",
    "\n",
    "- Principal Component Analysis (PCA): PCA is affected by the scales of the features because it maximizes the variance along the new axis.\n",
    "\n",
    "However, there are also algorithms where scaling isn't necessary:\n",
    "\n",
    "- Tree-based algorithms: Algorithms like Decision Trees, Random Forests, Gradient Boosting, and XGBoost do not require feature scaling. These algorithms are not distance-based and can handle various scales.\n",
    "\n",
    "- Naive Bayes: Naive Bayes is not affected by the feature scales as it is not distance based.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4bc99",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "The F1 score is a measure of a model's accuracy that considers both precision and recall. It's the harmonic mean of these two metrics. Precision refers to the percentage of your results which are relevant, while recall refers to the percentage of total relevant results correctly classified by your algorithm.\n",
    "\n",
    "The \"weighted\" F1 score means that each class's F1 score is weighted by the number of true instances for that class. This is useful in multi-class classification problems where you have an imbalanced dataset. That is, the number of instances of each class varies greatly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac9fa98",
   "metadata": {},
   "source": [
    "### Comparison of Logistic Regression, Random Forest, Support Vector Machines, and Naive Bayes methods:\n",
    "\n",
    " 1) Logistic Regression (LR):\n",
    "\n",
    "This is a statistical model used for binary classification problems (can be extended for multiclass problems).\n",
    "LR models the probabilities of class memberships as a logistic function of a linear combination of the predictors.\n",
    "It assumes a linear relationship between the features and the logit of the outcome, and it is parametric, meaning it makes assumptions about the data distribution.\n",
    "LR may struggle with complex non-linear data, and features should ideally be scaled before being fed into this algorithm.\n",
    "\n",
    "2) Random Forest (RF):\n",
    "\n",
    "This is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "RF can handle both numerical and categorical data, and it's good for high-dimensional spaces as well as large numbers of training examples.\n",
    "It does not require feature scaling, and it inherently provides a measure of feature importance.\n",
    "However, a large number of trees can make the model slow and ineffective for real-time predictions.\n",
    "\n",
    "3)  Support Vector Machines (SVM):\n",
    "\n",
    "This is a powerful and flexible class of supervised algorithms for both classification and regression.\n",
    "SVMs can handle linear and non-linear data well by applying a technique called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n",
    "It can handle high-dimensional data well, but it can be sensitive to overfitting depending on the kernel used and can struggle with larger datasets due to its computational complexity.\n",
    "SVM requires feature scaling for optimal performance.\n",
    "\n",
    "4)  Naive Bayes (NB):\n",
    "\n",
    "This is a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "It is simple and fast, and it's particularly suited for high-dimensional datasets.\n",
    "NB can handle both numerical and categorical data, and it works well even with less training data.\n",
    "However, the strong assumption of independent features (which is rarely true in real life) is a limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddad8b",
   "metadata": {},
   "source": [
    "## 1 Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical and machine learning algorithm often used for binary classification problems. Logistic regression is simple and computationally efficient, thats why its often used as a baseline. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc47574",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='lbfgs', random_state=42, max_iter=3000, n_jobs=-1) \n",
    "\n",
    "pipeline_params = {'ros': [ros, None], # upsampling or not\n",
    "              'scaler': [scaler, mms], # scaling input by standardizing or min-max scaling or not scaling at all\n",
    "              'classifier__C': loguniform(1e-2, 1e0),\n",
    "              'classifier__penalty': ['l2']} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d837b54",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"classifier\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de685f",
   "metadata": {},
   "source": [
    "### 1.2 Logistic Regression with Random Forest Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = RandomForestClassifier( random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"classifier\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb1c43",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = xgb.XGBClassifier(objective = \"multi:softmax\", num_class = 2,  random_state = 42, n_jobs =-1)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"classifier\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f646c",
   "metadata": {},
   "source": [
    "### 1.4 Logistic Regression with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce770d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pipeline_params = {'ros': [ros, None], # upsampling or not\n",
    "              'scaler': [scaler, mms], # scaling input by standardizing or min-max scaling or not scaling at all\n",
    "              'classifier__C': loguniform(1e-2, 1e0),\n",
    "              'classifier__penalty': ['l2'],\n",
    "              'pca__n_components': np.arange(4, 10, 1)} \n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"pca\", pca ],  [\"ros\", ros], [\"classifier\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb6aa3",
   "metadata": {},
   "source": [
    "## 2 Random Forest\n",
    "\n",
    "Random Forest is a popular and versatile machine learning method that is capable of performing both regression and classification tasks. It is also used for dimensionality reduction, treats missing values, outlier values, and other things.\n",
    "\n",
    "Random Forests generally have a high prediction accuracy and are quite efficient on large datasets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceee7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 42)\n",
    "pipeline_params = {\n",
    "    \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"rf__max_features\": [ \"sqrt\", \"log2\"],\n",
    "    \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "    \"rf__n_estimators\": np.array([ 50, 100, 200, 250]),\n",
    "    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f3517",
   "metadata": {},
   "source": [
    "### 2.1 Random Forest without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af277c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", model]]) #####?????? whats rf here?\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a23d9a",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest with Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88979c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = RandomForestClassifier(random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"rf\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193773d",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = xgb.XGBClassifier(objective = \"multi:softmax\", num_class = 2,  random_state = 42, n_jobs =-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"rf\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71ee8c",
   "metadata": {},
   "source": [
    "### 2.4 Random Forest with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174eef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pipeline_params = { \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "                    \"rf__max_features\": [ \"sqrt\", \"log2\"],\n",
    "                    \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "                    \"rf__n_estimators\": np.array([ 50, 100, 200, 250]),\n",
    "                    \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "                    'pca__n_components': np.arange(4, 10, 1)} \n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"pca\", pca ],  [\"ros\", ros], [\"rf\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6a3ba",
   "metadata": {},
   "source": [
    "## 3 Support Vector Machines\n",
    "\n",
    "\n",
    "Support Vector Machines (SVMs) aim to find a distinct hyperplane in a high-dimensional space to classify data points. They excel in managing high-dimensional data, even when dimensions outnumber samples, and are memory-efficient by leveraging a subset of training points called \"support vectors\" in decision-making. \n",
    "\n",
    "However, SVMs struggle with large feature sets compared to samples, requiring careful kernel and regularization term selection to avoid over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b29951",
   "metadata": {},
   "source": [
    "### 3.1 SVM without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SVC(random_state=42, max_iter= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5921506",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"SVM\", model]])\n",
    "pipeline_params = {\n",
    "    'ros': [ros, None], \n",
    "    \"SVM__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"SVM__C\": np.outer(np.logspace(-1, 1, 3),np.array([1,5])).flatten(),\n",
    "    \"SVM__gamma\": np.outer(np.logspace(-3, 0, 4),np.array([1,5])).flatten()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline(use_std=True)\n",
    "train.get_results(use_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84989728",
   "metadata": {},
   "source": [
    "### 3.2 SVM with RandomForest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a74aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = RandomForestClassifier( random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"SVM\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline(use_std=True)\n",
    "train.get_results(use_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452cc783",
   "metadata": {},
   "source": [
    "### 3.3 SVM with XgBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d77807",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = xgb.XGBClassifier(objective = \"multi:softmax\", num_class = 2,  random_state = 42, n_jobs =-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"SVM\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline(use_std=True)\n",
    "train.get_results(use_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0d91f",
   "metadata": {},
   "source": [
    "### 3.4 SVM with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"pca\", pca], [\"ros\", ros], [\"SVM\", model]])\n",
    "pipeline_params = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"SVM__C\": [ 10],\n",
    "    \"SVM__gamma\": [\"scale\"],\n",
    "    \"pca__n_components\": np.arange(4, 10, 1),\n",
    "}\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8bf24",
   "metadata": {},
   "source": [
    "## 4 Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf77e8",
   "metadata": {},
   "source": [
    "GaussianNB assumes that the likelihoods of different features in a class are independent from each other. Furthermore, the likelihoods in a class are assumed to be gaussian. Therefore, GaussianNB can only be applied to continous features. The probability to be in a specific class is estimated by the prior probabilities of the classes and the probabilities of the fitted gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c432c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB(random_state = 42)\n",
    "priors = (sum(y_train == 0)/y_train.shape[0], sum(y_train == 1)/y_train.shape[0])\n",
    "pipeline_param = {\n",
    "    \"scaler\": [scaler, mms, None],\n",
    "    \"ros\": [ros, None],\n",
    "    \"gnb__priors\": [None, priors],\n",
    "    \"gnb__var_smoothing\": np.logspace(0, -10, num = 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ce9d5",
   "metadata": {},
   "source": [
    "### 4.1 Naive Bayes without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"gnb\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc4046",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes with Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0826e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = RandomForestClassifier(random_state = 42)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"gnb\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c982d9",
   "metadata": {},
   "source": [
    "### 4.3 Naive Bayes with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = xgb.XGBClassifier(objective = \"multi:softmax\", num_class = 2,  random_state = 42, n_jobs =-1)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"gnb\", model]])\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef02eae",
   "metadata": {},
   "source": [
    "### 4.4 Naive Bayes with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"pca\", pca], [\"gnb\", model]])\n",
    "pipeline_params= {\n",
    "    \"scaler\": [scaler, mms, None], \n",
    "    \"gnb__priors\": [None, priors],\n",
    "    \"gnb__var_smoothing\": np.linspace(1, 0, num = 5),\n",
    "    \"pca__n_components\": np.arange(4, 10, 1)\n",
    "}\n",
    "\n",
    "train = TrainModel(model, None, pipe, pipeline_params)\n",
    "train.train_pipeline()\n",
    "train.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c706b1",
   "metadata": {},
   "source": [
    "## Part 4: Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13083f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.X, df.y\n",
    "stdCols = X.select_dtypes(include = \"float64\").columns #columns to be standardized\n",
    "X[stdCols] = (X[stdCols]-X[stdCols].mean())/X[stdCols].std()\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "batch_size = 4\n",
    "PATH = './net.pth'\n",
    "\n",
    "optimal_lr = 0.009\n",
    "optimal_momentum = 0.8\n",
    "optimal_dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3, input_dim = input_dim):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, input_dim//2)\n",
    "        self.lin2 = nn.Linear(input_dim//2, input_dim//4)\n",
    "        self.lin3 = nn.Linear(input_dim//4, 1)\n",
    "        self.drop1 = nn.Dropout(p = dropout_rate)\n",
    "        self.drop2 = nn.Dropout(p = dropout_rate)\n",
    "        self.prob = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.prob(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b689341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of Dataset for Dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.astype(\"int32\").values, dtype=torch.float32).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ac1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomDataloader(X, y):    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "    test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "\n",
    "    valloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCNetwork_train(network, trainloader, valloader, PATH, nr_epochs, print_running_loss):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=optimal_lr, momentum=optimal_momentum)\n",
    "    \n",
    "    min_val_loss = float(\"inf\")\n",
    "    for epoch in range(nr_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        n = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n += 1\n",
    "\n",
    "        if print_running_loss:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / n:.3f}')\n",
    "\n",
    "        network.eval()\n",
    "        val_loss = 0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                inputs, labels = data\n",
    "\n",
    "                outputs = network(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                n += 1\n",
    "\n",
    "        if print_running_loss:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {val_loss / n:.3f}')            \n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(network.state_dict(), PATH)\n",
    "            \n",
    "            if print_running_loss:   \n",
    "                print(f\"The new best model is at epoch {epoch}\")\n",
    "        \n",
    "        if print_running_loss:\n",
    "            print(f'Epoch: {epoch} over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c90fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_true_predicted(network, testloader, PATH): #, print_accuracy):\n",
    "    network.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = network(inputs)\n",
    "            predicted = outputs>0.5 \n",
    "            \n",
    "            y_true.extend(torch.reshape(labels, (batch_size, )).tolist())\n",
    "            y_pred.extend(torch.reshape(predicted, (batch_size, )).tolist())\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FCNetwork(input_dim = input_dim, dropout_rate = optimal_dropout)\n",
    "trainloader, valloader, testloader = CustomDataloader(X, y)\n",
    "FCNetwork_train(network, trainloader, valloader, PATH, nr_epochs=15, print_running_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f058b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_true_predicted(network, testloader, PATH)[0]\n",
    "y_pred = test_true_predicted(network, testloader, PATH)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(y_true, y_pred)\n",
    "print(class_report)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, normalize = \"true\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041dc6b",
   "metadata": {},
   "source": [
    "# Collection of important Figures from EDA and Feature Selection \n",
    "\n",
    "In the following, we want to explore the characteristics of the most important features in more detail. For this purpose, we apply an Extra Tree Classifier and show some histograms of the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset(path, years)\n",
    "df.do_eda(remove_duplicates=True, feature_eng=False, remove_corr=True, missing_thresh=0.3, reduce_col=['Sector'], reduce_thresh=0.75, imputer = \"median\")\n",
    "df.split_dataset()\n",
    "\n",
    "X, y = df.X, df.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "importances = model.feature_importances_\n",
    "importances_normalized = np.std([tree.feature_importances_ for tree in\n",
    "                                    model.estimators_],\n",
    "                                    axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e621c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = pd.Series(importances, index=X.columns).nlargest(200)\n",
    "n = len(top_features)\n",
    "\n",
    "# Plot feature importance of all features\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.bar(range(n), top_features[:n], align='center')\n",
    "\n",
    "plt.xlim([-1, n])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature Importance')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the importance of all features\n",
    "n = 15\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the most important features\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(n), top_features[:n], align='center')\n",
    "\n",
    "for i in range(n):\n",
    "    plt.text(i, top_features[i]+1e-4 , round(top_features[i], 3), ha='center', fontsize = 6)\n",
    "\n",
    "plt.xticks(range(n), top_features.index[:n], rotation=90)\n",
    "plt.xlim([-1, n])\n",
    "plt.ylim([0, 0.09])\n",
    "plt.title(f\"The {n} most important features\")\n",
    "plt.ylabel('Feature Importance')\n",
    "\n",
    "# Plot the 10 least important features\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(n), top_features[-n:], align='center')\n",
    "\n",
    "for i in range(n):\n",
    "    plt.text(i, top_features[len(top_features)-n+i]+1e-4 , round(top_features[len(top_features)-n+i], 3), ha='center', fontsize = 6)\n",
    "\n",
    "plt.xticks(range(n), top_features.index[-n:], rotation=90)\n",
    "plt.xlim([-1, n])\n",
    "plt.ylim([0, 0.09])\n",
    "plt.title(f\"The {n} least important features\")\n",
    "plt.ylabel('Feature Importance')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3)\n",
    "fig.set_size_inches(20, 10)\n",
    "\n",
    "#df = pd.concat((X, y), axis = 1)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    feat_cl0 = df[y == 0][top_features.index[i]]\n",
    "    feat_cl1 = df[y == 1][top_features.index[i]]\n",
    "    \n",
    "    mean_cl0 = feat_cl0.mean()\n",
    "    mean_cl1 = feat_cl1.mean()\n",
    "    \n",
    "    med_cl0 = feat_cl0.median()\n",
    "    med_cl1 = feat_cl1.median()    \n",
    "    \n",
    "    plot_range = (feat_cl0.quantile(0.05), feat_cl0.quantile(0.95))\n",
    "    ax.hist(feat_cl0, color = \"red\", bins = 100, alpha = 0.5, range = plot_range, label = \"Sell\")\n",
    "    ax.axvline(x = med_cl0, color = \"red\", linestyle='--', label = f\"median {round(med_cl0, 3)}\")\n",
    "    ax.axvline(x = mean_cl0, color = \"red\", linestyle=':', label = f\"mean {round(mean_cl0, 3)}\")\n",
    "    ax.hist(feat_cl1, color = \"green\", bins = 100, alpha = 0.5, range = plot_range, label = \"Buy\")\n",
    "    ax.axvline(x = med_cl1, color = \"green\", linestyle='--', label = f\"mean {round(med_cl1, 3)}\")\n",
    "    ax.axvline(x = mean_cl1, color = \"green\", linestyle=':', label = f\"median {round(med_cl1, 3)}\")\n",
    "    ax.set_xlabel(top_features.index[i])\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0abcb1d",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "With accuracies higher than 0.6, it seems that our models can predict the market beyond chance. However, the conclusion that it is possible to \"beat the market\" should not be drawn without further considerations. Firstly, since our data set is slightly imbalanced, one achieves an accuracy of 0.55 when predicting only values of one. More problematic, however, is the fact that we trained our models using the combined data set from 2014 to 2018. Our class variable is very unbalanced when considering the years individually. Therefore, it is possible that our algorithms learn the year of a measurement in the combined dataset and assign the majority class of that year to the measurement.\n",
    "\n",
    "To check if we can predict the stock market without any possible information of the year, we conclude our project by training Random Forrest Classifiers on different years seperately. The comparison of the accuracy score of the models trained on the different years reveals that the prediction is not strogly impeded due to the class imbalance. Furthermore, we conclude that the predictive power observed in the combined data set of the years 2014-2018 is independent of a hidden effect of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b0b22",
   "metadata": {},
   "source": [
    "### Accuracy with random prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy when all predictions are set to one: {round(accuracy_score(y_test, np.ones(len(y_test))),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "priors = (sum(y_test == 0)/y_test.shape[0], sum(y_test == 1)/y_test.shape[0])\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(10000):\n",
    "    y_pred = np.random.choice([0, 1], size = len(y_test), p = priors) \n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "fig = plt.hist(accuracies, bins = 50, label = f\"mean {round(np.mean(accuracies),2)} $\\pm$ {round(np.std(accuracies),2)}\")\n",
    "plt.xlabel(\"accuracy\")\n",
    "plt.xlim([0.4,0.6])\n",
    "plt.ylim([0,750])\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy with random prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23625076",
   "metadata": {},
   "source": [
    "### Class Imbalance in the years 2014-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a01aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = sys.path[0]\n",
    "data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "class0_years = []\n",
    "class1_years = []\n",
    "for year in range(2014,2019):\n",
    "    df_year = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "    class0_years.append(sum(df_year[\"Class\"] == 0))\n",
    "    class1_years.append(sum(df_year[\"Class\"] == 1))\n",
    "    \n",
    "N = 5\n",
    "ind = np.arange(N) # the x locations for the groups\n",
    "width = 0.35\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(ind, class0_years, width, color='r', label = \"Class 0 (Sell)\")\n",
    "ax.bar(ind, class1_years, width, bottom=class0_years, color='g', label = \"Class 1 (Buy)\")\n",
    "ax.set_ylabel('Occupancy')\n",
    "ax.set_title('Class occupancy in different years')\n",
    "ax.set_xticks(ind, [f\"{i}\" for i in range(2014,2019)])\n",
    "#ax.set_yticks(np.arange(0, 81, 10))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f285f30",
   "metadata": {},
   "source": [
    "### Random Forrest Classifier trained on the data from different years seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forrest Classifier fitted to the data of 2014\n",
    "for year in range(2014,2019):\n",
    "    df = Dataset(path, year)\n",
    "    df.do_eda(feature_eng=False, missing_thresh=0.3, reduce_thresh=0.75, imputer = \"median\")\n",
    "    df.split_dataset()\n",
    "    X_train, X_test, y_train, y_test = df.X_train, df.X_test, df.y_train, df.y_test\n",
    "\n",
    "    model = RandomForestClassifier(random_state = 42)\n",
    "    pipeline_params = {\n",
    "        \"rf__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"rf__max_features\": [ \"sqrt\", \"log2\"],\n",
    "        \"rf__max_depth\": np.array([None, 5, 10, 20]),\n",
    "        \"rf__n_estimators\": np.array([ 50, 100, 200, 250]),\n",
    "        \"rf__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "    }\n",
    "\n",
    "    pipe = imbpipeline(steps=[[\"ros\", ros], [\"rf\", model]])\n",
    "\n",
    "    train = TrainModel(model, None, pipe, pipeline_params)\n",
    "    print(f\"Random Forrest Classifier trained on data from {year}\\n\")\n",
    "    train.train_pipeline()\n",
    "    train.get_results(plot=False)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7bd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSLab",
   "language": "python",
   "name": "cslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
