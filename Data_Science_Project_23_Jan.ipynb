{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb2e6b6",
   "metadata": {},
   "source": [
    "# Data Science Project Spring 2023\n",
    "\n",
    "## 200+ Financial Indicators of US stocks (2014-2018)\n",
    "\n",
    "### Yiwei Gong, Janice Herman, Alexander  Morawietz and Selina Waber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1ae56",
   "metadata": {},
   "source": [
    "University of Zurich, Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b845b",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pandas_datareader import data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b155f9",
   "metadata": {},
   "source": [
    "## Loading the Data Set\n",
    "\n",
    "\n",
    "We used the data set from Nicolas Carbone from the webpage [kaggle](https://www.kaggle.com/datasets/cnic92/200-financial-indicators-of-us-stocks-20142018). Each dataset contains over 200 financial indicators, that are found in the [10-K filings](https://www.investopedia.com/terms/1/10-k.asp#:~:text=Key%20Takeaways-,A%2010%2DK%20is%20a%20comprehensive%20report%20filed%20annually%20by,detailed%20than%20the%20annual%20report.) of publicly traded companies from the US between the years 2014 - 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_directory = sys.path[0] ## get path of project directory\n",
    "data_directory = os.path.join(project_directory, 'data')\n",
    "\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "## Loading the yearly dataset into the array dfs\n",
    "dfs = []\n",
    "for year in years:\n",
    "    df = pd.read_csv(os.path.join(data_directory, f'{year}_Financial_Data.csv'), sep=',')\n",
    "    df['year'] = np.full(df.shape[0], str(year)) ## append column with the year respecitvely\n",
    "    df['PRICE VAR [%]'] = df[f'{year +1} PRICE VAR [%]'] ## Adding variable of the same name for all df, e.g. '2016 PRICE VAR [%]' renamed to 'PRICE VAR [%]'\n",
    "    df = df.drop(columns=[f'{year +1} PRICE VAR [%]']) # dropp year-specific variable name\n",
    "    df.columns.values[0] = 'Stock Name' # name the first variable \n",
    "    dfs.append(df)\n",
    "    \n",
    "    \n",
    "df = pd.concat(dfs, ignore_index=True) ## concat the diffrent dataframes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d5a87",
   "metadata": {},
   "source": [
    "## Some Explanation of Variables:\n",
    "\n",
    "### Adding  `year` as a cathegorical variable\n",
    "\n",
    "We added a column named `year` which contains the respecitve year.\n",
    "\n",
    "\n",
    "\n",
    "### Handling the variable `Price VAR [%]`\n",
    "\n",
    "The last column, `PRICE VAR [%]`, lists the percent price variation of each stock for the year. For example, if we consider the dataset 2015_Financial_Data.csv, we will have:\n",
    "\n",
    "- 200+ financial indicators for the year 2015;\n",
    "- percent price variation for the year 2016 (meaning from the first trading day on Jan 2016 to the last trading day on Dec 2016).\n",
    "\n",
    "We renamed all the variables with the specific year in it, e.g.  `2016 PRICE VAR [%]` to `PRICE VAR [%]`. We dropped the old ones.Now we just have one variable `PRICE VAR [%]`. \n",
    "\n",
    "\n",
    "### The variable `class`\n",
    "\n",
    "class lists a binary classification for each stock, where\n",
    "\n",
    "- for each stock, if the PRICE VAR [%] value is positive, class = 1. From a trading perspective, the 1 identifies those stocks that an hypothetical trader should BUY at the start of the year and sell at the end of the year for a profit.\n",
    "- for each stock, if the PRICE VAR [%] value is negative, class = 0. From a trading perspective, the 0 identifies those stocks that an hypothetical trader should NOT BUY, since their value will decrease, meaning a loss of capital.\n",
    "\n",
    "\n",
    "The columns `PRICE VAR [%]` and `class` make possible to use the datasets for both classification and regression tasks:\n",
    "\n",
    "- If the user wishes to train a machine learning model so that it learns to classify those stocks that in buy-worthy and not buy-worthy, it is possible to get the targets from the class column;\n",
    "- If the user wishes to train a machine learning model so that it learns to predict the future value of a stock, it is possible to get the targets from the PRICE VAR [%] column.\n",
    "\n",
    "\n",
    "### The variable  `Stock Name`\n",
    "\n",
    "We named the first variable `Stock Name`since it has not been named in the original dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6629f",
   "metadata": {},
   "source": [
    "## First Description of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd87fb",
   "metadata": {},
   "source": [
    "## Numerical and Catgorical Features/Variables\n",
    "\n",
    "We are converting `Class`to a cathegorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'] = df.Class.astype('object') ## object or catheogry?? whats the difference???\n",
    "\n",
    "numCols = df.select_dtypes(exclude='object').columns\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\")\n",
    "\n",
    "catCols = df.select_dtypes(include='object').columns\n",
    "print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbd0e7",
   "metadata": {},
   "source": [
    "## Any Duplicates? \n",
    "\n",
    "No, there are no duplicates for rows but there are 20 duplicates for columns/ 10 each. Not same variable name but same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cac5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in Rows:', True in list(df.duplicated()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates in Columns:', True in list(df.T.duplicated().T))\n",
    "print(\"Show the Duplicates:\")\n",
    "print(df.T[df.T.duplicated(keep=False)].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38ff15",
   "metadata": {},
   "source": [
    "Our Duplicates are the following pairs:\n",
    "\n",
    "- `ebitperRevenue` and `eBITperRevenu`\n",
    "- `ebtperEBIT` and `eBTperEBIT`\n",
    "- `niperEBT` and `nIperEBT`\n",
    "- `returnOnAssets` and `Return on Tangible Assets`\n",
    "- `returnOnCapitalEmployed` and `ROIC`\n",
    "- `payablesTurnover` and `Payables Turnover`\n",
    "- `inventoryTurnover` and `Inventory Turnover`\n",
    "- `debtRatio` and `Debt to Assets`\n",
    "- `debtEquityRatio` and `Debt to Equity`\n",
    "- `cashFlowToDebtRatio` and `cashFlowCoverageRatios`\n",
    "\n",
    "We will remove the first occurence of the duplicates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_old=df.shape\n",
    "\n",
    "## HUGHE PROBLEM --> df.drop_duplicates() transforms all variables to the datatype 'object'!\n",
    "#I don't know why it does that!\n",
    "# So i will remove the manually!!!\n",
    "\n",
    "#df= df.T.drop_duplicates().T # remove duplicates!\n",
    "\n",
    "df= df.drop(columns=['eBITperRevenue', 'eBTperEBIT', 'nIperEBT', 'Return on Tangible Assets', \n",
    "                     'ROIC', 'Payables Turnover', 'Inventory Turnover', 'Debt to Assets', 'Debt to Equity', \n",
    "                     'cashFlowCoverageRatios'])\n",
    "\n",
    "print(f' Shape with duplicates:', shape_old) \n",
    "print(f' Shape after removal of duplicates:', df.shape) \n",
    "\n",
    "\n",
    "#print(df.info(verbose=True)) ok good, sill folat64 objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07f7e9",
   "metadata": {},
   "source": [
    "## Variation \n",
    "There is something seriously wrong with the variable `operatingProfitMargin`. It has a Standard Deviation of 0, which cant be! So we dropp it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.std(axis=0, skipna=True, numeric_only=True)\n",
    "\n",
    "no_std= [ std for std in df.std( axis=0, skipna=True, numeric_only=True) if std <= 0.15]\n",
    "print(no_std)\n",
    "\n",
    "\n",
    "\n",
    "#x = df.std( axis = 0, skipna = True, numeric_only = True)\n",
    "#[(index, x[index]) for index in x.index.values if x[index] > 0]\n",
    "\n",
    "### TO DOOO!!!!!!!!!!!!!!!!!\n",
    "\n",
    "df= df.drop(columns= ['operatingProfitMargin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb053bda",
   "metadata": {},
   "source": [
    "Furthermore, there are two variables (operatingCycle and cashConversionCycle) which have missing values in more than 99% of the time. These two columns are also dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa301fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_nan_columns = df.isnull().mean(axis = 0)\n",
    "\n",
    "print(\"Variables with maximal missing rate:\")\n",
    "print(most_nan_columns[most_nan_columns > 0.5])\n",
    "df = df.drop(columns = [\"operatingCycle\", \"cashConversionCycle\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0c759",
   "metadata": {},
   "source": [
    "## Correlation of the variables\n",
    "\n",
    "Although we removed all doublicates there are still many variables that are highly correlated. We consider variables with correlations larger than 0.98 as doublicates and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0eb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_corr contains the predictor variables which have to be checked for high correlations.\n",
    "check_corr = df.drop(labels=[\"Class\", \"Stock Name\", \"Sector\", \"year\", \"PRICE VAR [%]\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(check_corr.corr().abs())\n",
    "plt.title(\"Correlations between the predictors\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c79590",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_corr = check_corr.corr().abs()\n",
    "for i in range(len(abs_corr)):\n",
    "    abs_corr.iloc[i, i] = 0\n",
    "    \n",
    "    \n",
    "abs_corr_unstack = abs_corr.unstack()\n",
    "print(\"Variable pairs with high correlation\")\n",
    "print(abs_corr_unstack.sort_values(kind=\"quicksort\")[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A variables is removed if the correlation with another variable is higher than 0.98.\n",
    "\n",
    "columns_to_drop = []\n",
    "columns_to_remain = []\n",
    "\n",
    "for pair in abs_corr_unstack.index.values:\n",
    "    if abs_corr_unstack[pair] > 0.98:\n",
    "        if pair[0] not in columns_to_remain and pair[1] not in columns_to_remain:\n",
    "                columns_to_remain.append(pair[0])\n",
    "                if pair[1] not in columns_to_drop:\n",
    "                    columns_to_drop.append(pair[1])\n",
    "        elif pair[0] in columns_to_remain:\n",
    "            if pair[1] not in columns_to_drop:\n",
    "                columns_to_drop.append(pair[1])\n",
    "        elif pair[1] in columns_to_remain:\n",
    "            if pair[0] not in columns_to_drop:\n",
    "                columns_to_drop.append(pair[0])\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(columns_to_drop)} variables were removed as dublicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49a06f",
   "metadata": {},
   "source": [
    "## Class Balance\n",
    "\n",
    "The Variable `Class`is not balanced. We have to keep that in mind for train and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The class variable is 1 in {df['Class'].value_counts()[1]} cases.\")\n",
    "print(f\"The class variable is 0 in  {df['Class'].value_counts()[0]} cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4169a0c0",
   "metadata": {},
   "source": [
    "## Description of Outliers\n",
    "\n",
    "In our data set, we observe several columns containing values that are multiple orders larger than the 75% percentil or smaller than the 25% percentil. For example, the maximum value of the variable \"revenue\" is 1.9e12 while the 75% percentil is 2.3e9. However, it is possible that a large company has an exceding revenue compared to others. Therefore, it is not trivial to decide if such points are errors or reflect a true value.\n",
    "\n",
    "As shown below, several outliers are observed in the variable \"PRICE VAR [%]\". As the dependet variable, its outliers are likey to strongly bias the statistical models. Therefore, the variable \"PRICE VAR [%]\" is analysed in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc81ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39096800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.loc[:, ['Sector','PRICE VAR [%]']]\n",
    "\n",
    "# Get list of sectors\n",
    "sector_list = df_['Sector'].unique()\n",
    "\n",
    "# Plot the percent price variation for each sector\n",
    "for sector in sector_list:\n",
    "    \n",
    "    temp = df_[df_['Sector'] == sector]\n",
    "\n",
    "    plt.figure(figsize=(30,5))\n",
    "    plt.plot(temp['PRICE VAR [%]'])\n",
    "    plt.title(sector.upper())\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stocks that increased more than 500%\n",
    "gain = 500\n",
    "top_gainers = df[df['PRICE VAR [%]'] >= gain]\n",
    "top_gainers = top_gainers['PRICE VAR [%]'].sort_values(ascending=False)\n",
    "print(f'{len(top_gainers)} STOCKS with more than {gain}% gain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee2c7d",
   "metadata": {},
   "source": [
    "## Cleaning of Outliers\n",
    "As described above, our data set contains many outliers which could reflect wrong data point, e.g. due to mistyping. Since some of our models are sensitive to outliers, we decided to replace the most extreme 2% of our data with the 0.99 quantile or with the 0.01 quantile, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_quantiles = df.quantile(0.99)\n",
    "outliers_top = (df > top_quantiles)\n",
    "\n",
    "low_quantiles = df.quantile(0.01)\n",
    "outliers_low = (df < low_quantiles)\n",
    "\n",
    "df = df.mask(outliers_top, top_quantiles, axis=1)\n",
    "df = df.mask(outliers_low, low_quantiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z-Score: \n",
    "\n",
    "threshold = 3\n",
    "\n",
    "test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "for col in test:\n",
    "    z_score= stats.zscore(df[col], nan_policy='omit')\n",
    "    outlier_indices = np.where(z_score > threshold)\n",
    "    print(outlier_indices )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6623d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR\n",
    "test= df[['ebtperEBIT', 'returnOnCapitalEmployed']]\n",
    "for col in test:\n",
    "    iqr= stats.iqr(df[col], nan_policy='omit')\n",
    "    print(iqr)\n",
    "\n",
    "#''''''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faf4a8",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eac6a",
   "metadata": {},
   "source": [
    "## Description of Missing Data\n",
    "\n",
    "In our data set, we observe many missing values. There are several columns with more than 30% of NaN values. In the following, a short description of these columns is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are in total {df.isnull().sum().sum()} NAN in the dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11afbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.replace([np.inf, -np.inf], np.nan, inplace=True) do that in Outliers Section!\n",
    "#df.replace(to_replace = 0, value = np.nan, inplace=true) ????? DA EVTL noch mehr mit NAN ersetzen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview of all variables with missing values\n",
    "df.isnull().mean().sort_values(ascending=False).plot.bar(figsize=(100,20))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.xlabel('Variables')\n",
    "plt.title('Quantifying ALL missing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a32640",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_nan = df.isnull().mean().sort_values(ascending=False)\n",
    "most_nan = most_nan[most_nan > 0.3]\n",
    "\n",
    "most_nan.plot.bar(figsize=(8,4))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.title('Columns with more than 30% missing values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "missing_data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e4015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values 2.0\n",
    "sns.heatmap(df.isna().transpose(), cmap=\"Blues\", cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a203a7",
   "metadata": {},
   "source": [
    "It is conspicuous that there are columns with more than 50% of the values equal to zero. It is likely that these cases reflect missing values. Therefore, we decided to consider them as missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_zero = df.drop(columns = \"Class\").isin([0]).mean().sort_values(ascending=False)\n",
    "most_zero = most_zero[most_zero > 0.5]\n",
    "\n",
    "most_zero.plot.bar(figsize=(8,4))\n",
    "plt.ylabel('Percentage of missing values')\n",
    "plt.title('Columns with more than 50% zero values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2db1d",
   "metadata": {},
   "source": [
    "## Cleaning of Missing Data\n",
    "\n",
    "To remove the missing values, we decided to drop all columns with more than 30% of NaNs. Furthermore, we dropped all columns with more than 50% of zero values. The remaining NaN fields are replaced by the median of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84362258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all columns with nr. of NaN > 30%\n",
    "\n",
    "print(\"Shape of df data:\", df.shape)\n",
    "\n",
    "df = df.drop(columns=most_nan.index.values)\n",
    "\n",
    "print(\"Shape of df data after removing columns with nr. of NaN > 30%:\", df.shape)\n",
    "\n",
    "df = df.drop(columns=most_zero.index.values)\n",
    "\n",
    "print(\"Shape of df data after removing columns with nr. of zeros > 50%:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(\"New numerical columns:\", numCols)\n",
    "df[numCols] = df[numCols].fillna(df[numCols].median())\n",
    "\n",
    "catCols = df.select_dtypes(exclude=np.number).columns\n",
    "print(\"New categorical columns:\", catCols)\n",
    "for col in catCols:\n",
    "    df[col].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68073cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Missing data again\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Percent of Missing Values'])\n",
    "missing_data.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e92f5",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ece60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "df = pd.get_dummies(df, columns=catCols.drop(\"Stock Name\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85406b",
   "metadata": {},
   "source": [
    "## Assigning X and target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels=[\"Class_0\", \"Class_1\", \"Stock Name\", \"year_2014\", \"year_2015\", \"year_2016\", \"year_2017\", \"year_2018\", \"PRICE VAR [%]\"], axis = 1)\n",
    "y = df[\"Class_1\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b031cee",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47596a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a8a80",
   "metadata": {},
   "source": [
    "## Important Link\n",
    "\n",
    "[Cleaning Data Set](https://www.kaggle.com/code/cnic92/explore-and-clean-financial-indicators-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b0a4c",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f79be",
   "metadata": {},
   "source": [
    "As a last step in the exploratory data analysis we will use an Extra Trees Classifier to extract the most important features of our data set. This feature selection method will be included later in our pipeline. At this point, we want to anaylse the most important features in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d373596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting most significant features\n",
    "\n",
    "# Feature selection using Extra Trees Classifier on the resampled training data\n",
    "model = ExtraTreesClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Select top features with highest importance scores\n",
    "top_features = pd.Series(importances, index=X.columns).nlargest(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the importance of all features\n",
    "n = len(top_features)\n",
    "\n",
    "# Plot feature importance of all features\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(n), top_features[:n], align='center')\n",
    "\n",
    "plt.xlim([-1, n])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature Importance')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf25056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the importance of all features\n",
    "n = 15\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the most important features\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(range(n), top_features[:n], align='center')\n",
    "\n",
    "for i in range(n):\n",
    "    plt.text(i, top_features[i]+1e-4 , round(top_features[i], 3), ha='center', fontsize = 6)\n",
    "\n",
    "plt.xticks(range(n), top_features.index[:n], rotation=90)\n",
    "plt.xlim([-1, n])\n",
    "plt.ylim([0, 0.015])\n",
    "plt.title(f\"The {n} most important features\")\n",
    "plt.ylabel('Feature Importance')\n",
    "\n",
    "# Plot the 10 least important features\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.bar(range(n), top_features[-n:], align='center')\n",
    "\n",
    "for i in range(n):\n",
    "    plt.text(i, top_features[len(top_features)-n+i]+1e-4 , round(top_features[len(top_features)-n+i], 3), ha='center', fontsize = 6)\n",
    "\n",
    "plt.xticks(range(n), top_features.index[-n:], rotation=90)\n",
    "plt.xlim([-1, n])\n",
    "plt.ylim([0, 0.01])\n",
    "plt.title(f\"The {n} least important features\")\n",
    "plt.ylabel('Feature Importance')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd971dd",
   "metadata": {},
   "source": [
    "It is interesting to note that the least important feature is the categorial variable \"Sector\". Furthermore, it is not surprising that the most important features are EPS (\"earnings per share\") or revenue growth (either averaged over one, three or five years) which are closely connected with the price variance of the share. However, many variables measure almost identical finacial indicators and are therefore highly correlated. Therefore, the importances are similar and a choice of a subset of featured for a reduced data set is rather arbitrary. Nevertheless, a description of the 10 most important variables is given in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter_matrix of the 10 most important features\n",
    "\n",
    "n=10\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "axes = pd.plotting.scatter_matrix(X[top_features.index[:n]], alpha=0.2)\n",
    "for ax in axes.flatten():\n",
    "    #ax.xaxis.label.set_rotation(90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.yaxis.label.set_ha('right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gcf().subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fde749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependence between the most important variable and PRICE VAR [%]\n",
    "\n",
    "plt.scatter(X[top_features.index[0]], df[\"PRICE VAR [%]\"])\n",
    "plt.xlabel(top_features.index[0])\n",
    "plt.ylabel(\"PRICE VAR [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c29284",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5,2)\n",
    "fig.set_size_inches(20, 20)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    feat_cl0 = df[y == 0][top_features.index[i]]\n",
    "    feat_cl1 = df[y == 1][top_features.index[i]]\n",
    "    \n",
    "    mean_cl0 = feature_class0.mean()\n",
    "    mean_cl1 = feature_class1.mean()\n",
    "    \n",
    "    med_cl0 = feature_class0.median()\n",
    "    med_cl1 = feature_class1.median()    \n",
    "    \n",
    "    plot_range = (feat_cl0.quantile(0.05), feat_cl0.quantile(0.95))\n",
    "    ax.hist(feat_cl0, color = \"red\", bins = 100, alpha = 0.5, range = plot_range, label = \"Sell\")\n",
    "    ax.axvline(x = med_cl0, color = \"red\", linestyle='--', label = f\"median {round(med_cl0, 3)}\")\n",
    "    ax.axvline(x = mean_cl0, color = \"red\", linestyle=':', label = f\"mean {round(mean_cl0, 3)}\")\n",
    "    ax.hist(feat_cl1, color = \"green\", bins = 100, alpha = 0.5, range = plot_range, label = \"Buy\")\n",
    "    ax.axvline(x = med_cl1, color = \"green\", linestyle='--', label = f\"mean {round(med_cl1, 3)}\")\n",
    "    ax.axvline(x = mean_cl1, color = \"green\", linestyle=':', label = f\"median {round(med_cl1, 3)}\")\n",
    "    ax.set_xlabel(top_features.index[i])\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dac58b",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report)\n",
    "\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c196b",
   "metadata": {},
   "source": [
    "The predictor variables for the neural network include all the numerical variables and the categorial dummy variables \"Sector_x\". The numerical variables are standardized by the z-score standardization. The network is trained on the binary variable \"Class\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = X.copy()\n",
    "stdCols = X_std.select_dtypes(include = \"float64\").columns #columns to be standardized\n",
    "X_std[stdCols] = (X_std[stdCols]-X_std[stdCols].mean())/X[stdCols].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310adef",
   "metadata": {},
   "source": [
    "### FCNetwork\n",
    "\n",
    "We build a fully connected network with two hidden layers. To prevent overfitting a dropout layer follows both the hidden layers. We use the rectified linear unit function for activation. The output is transformed by the sigmoid function to find the predicted values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c277ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3, input_dim = input_dim):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, input_dim//2)\n",
    "        self.lin2 = nn.Linear(input_dim//2, input_dim//4)\n",
    "        self.lin3 = nn.Linear(input_dim//4, 1)\n",
    "        self.drop1 = nn.Dropout(p = dropout_rate)\n",
    "        self.drop2 = nn.Dropout(p = dropout_rate)\n",
    "        self.prob = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.prob(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938f7ff",
   "metadata": {},
   "source": [
    "### Grid Search for optimal Hyperparameters\n",
    "To improve the accuracy of the model, the hyperparameters \"learning rate\", \"momentum\" and \"dropout rate\" are optimized by grid search. This is performed by the function Gridsearch which uses GridSearchCV from sklearn.model_selection and returns the optimal parameters from the grid \"param_grid\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation of the data to tensors for the Grid Search\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.astype(\"int32\").values, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"optimizer__lr\": [0.001, 0.004, 0.007, 0.01], \n",
    "              \"optimizer__momentum\": [0.6, 0.7, 0.8, 0.9],\n",
    "              \"module__dropout_rate\": [0.2, 0.3, 0.4, 0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67823cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(X, y, param_grid, max_epochs, batch_size):\n",
    "    #Transformation of the data to tensors for the Grid Search\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.astype(\"int32\").values, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    model = NeuralNetClassifier(\n",
    "    FCNetwork,\n",
    "    criterion=nn.BCELoss,\n",
    "    optimizer=optim.SGD,\n",
    "    max_epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=False\n",
    "    )\n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs = -1)\n",
    "    grid_result = grid.fit(X_tensor, y_tensor)\n",
    "    \n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "    return grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225248e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "optimal_hyperp = GridSearch(X, y, param_grid, max_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661df89",
   "metadata": {},
   "source": [
    "### Training the Neural Network with the optimal parameters\n",
    "The optimal hyperparameters were found to be:\n",
    "    learning rate = 0.01,\n",
    "    momentum = 0.9,\n",
    "    dropout rate = 0.2.\n",
    "   \n",
    "With these hyperparameters and a batch size of 4 the neural network is trained using pytorch. For this purpose, a class CustomDataset is created which transforms the data to torch.tensors and contains a method __get_item__(self, idx) for the data loader. The dataloader is defined in the function CustomDataloader(X, y). It splits the data set in parts for training, evaluation and testing. By using the class DataLoader from torch.utils.data it returns the data set in the appropriate data type for the FCNetwork. The training is performed by the function FCNetwork_train(network, trainloader, valloader, PATH, nr_epochs, print_running_loss) which saves the optimal network parameters in a file under PATH. The function test_true_predicted(network, testloader, PATH): returns a list of the true class labels and a list of the predicted class labels from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lr = 0.01 #GridSearch(X, y, param_grid, max_epochs, batch_size)[\"optimizer__lr\"]\n",
    "optimal_momentum = 0.9 #GridSearch(X, y, param_grid, max_epochs, batch_size)[\"optimizer__momentum\"]\n",
    "optimal_dropout = 0.2 #GridSearch(X, y, param_grid, max_epochs, batch_size)[\"module__dropout_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of Dataset for Dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.astype(\"int32\").values, dtype=torch.float32).reshape(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomDataloader(X, y):    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 42) \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "    test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                              shuffle=True)\n",
    "\n",
    "    valloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc59c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCNetwork_train(network, trainloader, valloader, PATH, nr_epochs, print_running_loss):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(network.parameters(), lr=optimal_lr, momentum=optimal_momentum)\n",
    "    \n",
    "    min_val_loss = float(\"inf\")\n",
    "    for epoch in range(nr_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        network.train()\n",
    "        n = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n += 1\n",
    "\n",
    "        if print_running_loss:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / n:.3f}')\n",
    "\n",
    "        network.eval()\n",
    "        val_loss = 0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                inputs, labels = data\n",
    "\n",
    "                outputs = network(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                n += 1\n",
    "\n",
    "        if print_running_loss:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {val_loss / n:.3f}')            \n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(network.state_dict(), PATH)\n",
    "            \n",
    "            if print_running_loss:   \n",
    "                print(f\"The new best model is at epoch {epoch}\")\n",
    "        \n",
    "        if print_running_loss:\n",
    "            print(f'Epoch: {epoch} over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_true_predicted(network, testloader, PATH): #, print_accuracy):\n",
    "    network.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    #correct = 0\n",
    "    #total = 0\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = network(inputs)\n",
    "            predicted = outputs>0.5 \n",
    "            \n",
    "            y_true.extend(torch.reshape(labels, (batch_size, )).tolist())\n",
    "            y_pred.extend(torch.reshape(predicted, (batch_size, )).tolist())\n",
    "            \n",
    "            #total += labels.size(0)\n",
    "            #correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    #if print_accuracy:\n",
    "    #print(f'Accuracy of the network on the test images: {100 * correct / total} %')\n",
    "    return y_true, y_pred #correct / total, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './net.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FCNetwork(input_dim = input_dim, dropout_rate = optimal_dropout)\n",
    "trainloader, valloader, testloader = CustomDataloader(X_std, y)\n",
    "FCNetwork_train(network, trainloader, valloader, PATH, nr_epochs=10, print_running_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_true_predicted(network, testloader, PATH)[0]\n",
    "y_pred = test_true_predicted(network, testloader, PATH)[1]\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Full model\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd27038",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize = (8,5))\n",
    "sn.heatmap(cf_matrix/len(y_true), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584232b",
   "metadata": {},
   "source": [
    "To analyze the significance of the different predictor variables, we train different models, omitting one of the predictor variables at a time. While the accuracy of the model trained on the full data set is approximately 0.59, the accuracy of several models trained on a reduced data set drops below 0.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dropped_variable = []\n",
    "\n",
    "for column in X_std.columns.values:\n",
    "    \n",
    "    network = FCNetwork(input_dim = input_dim-1, dropout_rate = optimal_dropout)\n",
    "    \n",
    "    trainloader, valloader, testloader = CustomDataloader(X_std.drop(columns = column), y)\n",
    "\n",
    "    FCNetwork_train(network, trainloader, valloader, PATH, nr_epochs=10, print_running_loss=False)\n",
    "        \n",
    "    y_true = test_true_predicted(network, testloader, PATH)[0]\n",
    "    y_predicted = test_true_predicted(network, testloader, PATH)[1]\n",
    "\n",
    "    acc = accuracy_score(y_true, y_predicted)\n",
    "   \n",
    "    accuracy_dropped_variable.append(acc)\n",
    "    \n",
    "    print(column, acc)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85026761",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dropped_variable = pd.Series(accuracy_dropped_variable, index = X.columns.values)\n",
    "accuracy_dropped_variable = accuracy_dropped_variable.sort_values(axis = 0)\n",
    "accuracy_dropped_variable.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37295916",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25,10))\n",
    "barplot = plt.bar(influence.index.values, influence)\n",
    "xticks = plt.xticks(influence.index.values, rotation='vertical', fontsize=5)\n",
    "plt.ylim((0.54, 0.6))\n",
    "plt.plot([0.58 for i in range(len(accuracy_with_dropped_variable))], linestyle='dashed', color = \"black\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"name of dropped variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_influencial_variables = accuracy_dropped_variable[accuracy_dropped_variable < 0.58].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344cb00",
   "metadata": {},
   "source": [
    "## Testing Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier)\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f937093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting most significant features\n",
    "\n",
    "# Feature selection using Extra Trees Classifier on the resampled training data\n",
    "model = ExtraTreesClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Select top features with highest importance scores\n",
    "top_features = pd.Series(importances, index=X_train.columns).nlargest(300)\n",
    "\n",
    "# Subset X_resampled and X_test with selected features\n",
    "X_train_selected = X_train[top_features.index]\n",
    "X_test_selected = X_test[top_features.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07151e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with the best hyperparameters found\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=1670,\n",
    "    criterion='gini',\n",
    "    max_depth=18,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='sqrt',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=False,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the Random Forest model with the best hyperparameters\n",
    "rf_best.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict the test set\n",
    "y_pred_rf = rf_best.predict(X_test_selected)\n",
    "\n",
    "# Model evaluation\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "prec_rf = precision_score(y_test, y_pred_rf, average='macro')\n",
    "rec_rf = recall_score(y_test, y_pred_rf, average='macro')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
    "print(\"Random Forest Classifier: Accuracy = %.3f, Precision = %.3f, Recall = %.3f, F1 Score = %.3f\" % (acc_rf, prec_rf, rec_rf, f1_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n",
    "print(y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253531f8",
   "metadata": {},
   "source": [
    "## Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create StandardScaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Standardize features; equal results as if done in two\n",
    "X_train_std = sc.fit_transform(X_train_selected)\n",
    "X_test_std = sc.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for getting optimal C and gamma\n",
    "gamma_range = np.outer(np.logspace(-3, 0, 4),np.array([1,5]))\n",
    "gamma_range = gamma_range.flatten()\n",
    "print(gamma_range)\n",
    "\n",
    "C_range = np.outer(np.logspace(-1, 1, 3),np.array([1,5]))\n",
    "C_range = C_range.flatten()\n",
    "print(C_range)\n",
    "\n",
    "parameters = {'kernel':['linear', 'rbf'], 'C':C_range, 'gamma': gamma_range}\n",
    "\n",
    "svm = SVC()\n",
    "grid = RandomizedSearchCV(estimator=svm, param_distributions=parameters, n_iter=5, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train_std, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(grid.best_score_))\n",
    "print('Test score:       {:.2f}'.format(grid.score(X_test_std, y_test)))\n",
    "print('Best parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c272b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSLab",
   "language": "python",
   "name": "cslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
